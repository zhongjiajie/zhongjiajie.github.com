---
title: Airflow 用户指南
date: 2019-04-05 19:35:36
categories:
- Airflow
---

# Airflow 用户指南

Airflow用户指南,基于[我的wiki][1]提炼出来的内容,简单描述了使用Airflow过程中有什么注意事项及需要关注的点

<!-- more -->

## 依赖关系

**根据[committer成员的讨论][21],更加推荐使用位操作符(bitwise operators)来解决依赖**

* 最原始的方式`task.set_upstream(task1); task.set_downstream(task2)`
* 位操作(bitwise operators)方式,airflow1.8之后`task >> task1; task << task1; task >> task1 << task2`
  * DAG的位操作符号可以提供更多的功能: `t1 >> [t2, t3] >> t4`已经能被支持了
* ~~使用`chain`方法实现多个task的依次依赖~~(Airflow 1.10.3已经取消`chain`方法)
  * ~~chain的一般使用~~

    ```py
    from airflow.utils.helpers import chain
    chain(task, task1, task2)
    ```

  * ~~通过列表解析直接生成task列表然后chain起来~~

    ```py
    from airflow.utils.helpers import chain
    ds_true = [DummyOperator(task_id='true_' + str(i), dag=dag) for i in [1, 2]]
    chain(cond_true, *ds_true)
    ```

* 一对多的链接关系
  * `t1 >> [t2, t3]`(推荐)
  * `group = [task1, task2, task3]; task.set_downstream(group);`
* 多对一的链接关系
  * `[t1, t2] >> t3`(推荐)
  * `group = [task1, task2, task3]; task.set_upstream(group)`
* 多对多的笛卡尔积
  * `airflow.utils.helper.cross_downstream([t1, t2, t3], [t4, t5, t6])`(推荐)
  * 使用自定义的方法

    ```py
    import itertools
    import airflow.utils.helper.chain
    group_a=[task1, task2, task3]
    group_b=[task4, task5]
    for pair in itertools.product(group_a, group_b):
        chain(*pair)
    ```

* 如果有根据一定条件选择下游执行哪个task操作的逻辑,可以使用`BranchPythonOperator`算子,使用是可以通过`TriggerRule.ONE_SUCCESS`设置实现.例如[例子][11]A是`BranchPythonOperator`,一个分支运行B,另一个分支运行C,同时B->C,这时可以在C中设置`TriggerRule.ONE_SUCCESS`.以前我总认为一个该[这样][12]实现,会多个两个算子

## 时间相关

* 使用Airflow内置的日期宏
  * airflow内置了部分时间相关的参数,如`{% raw %} '{{ ds }}' {% endraw %}`
代表运行的时间,`{% raw %} '{{ yesterday_ds }}' {% endraw %}`
运行时间昨天的日期,更多时间相关的参数见[这里][24]
* 自定义时间参数
  * 简单的时间操作: 通过`replace`完成,获取运行日期同时改变成特殊的时间`{% raw %} some_command.sh {{ execution_date.replace(day=1) }} {% endraw %}`
  * 通过`macros`对时间进行更多操作: `macros.ds_add`将内置时间进行计算`{% raw %} '{{ macros.ds_add(ds, 1) }}' {% endraw %}`

## 资源限制

作为一个工作流工具,除了完成各种复杂的上下游关系外,我认为解决资源的限制也是很重要的点.资源限制包括限制DAG的并发,限制多个DAG的运行关系.限制同一个/同一类Task的并发

### DAG的限制

* 限制dag并行实例数量
  * 在`airflow.cfg`的`[core]`设置`dag_concurrency`限制并行数量
  * 在DAG文件中限制

    ```py
    from airflow import DAG

    default_args = {
        'owner': 'airflow',
        # here to set value
        'concurrency': 10
    }

    dag = DAG(
        'tutorial',
        default_args=default_args,
        description='A simple tutorial DAG',
        schedule_interval=timedelta(days=1),
    )
    ```

### Task的限制

* 限制task的并行数量: operetor中的参数`task_concurrency`可以设置task的并行数量
* 限制多个不同类型的task并行数量: operetor中的参数`pool`限制一类task的并行数量,与`task_concurrency`参数的区别是`task_concurrency`设置的是同一个task的并行数task_id要相同,`pool`设置的是一类task的并行数task_id可以不同,只要保证`pool`参数的名称相同就可以.设置后并行的task不会超过`pool`对象的`slots`值

## 将Airflow中的对象通过DAG或者脚本的方式进行保存

### connection variables pool

目前Airflow创建connection variables pool能通过如下方式创建:

* Airflow的cli命令创建,对应命令分别为: `airflow connections --add` `airflow variables -s` `airflow pools -s`
* Airflow的web UI页面进行设置,分别为: `Admin -> connections` `Admin -> variables` `Admin -> pools`

这里提供一个将connection variables pool固定到DAG的方法,查看[这里][18].主要是之前使用docker-airflow每次重启时都会清空postgre数据,这样能保证connection variables pool能被git进行版本管理,下面以`connections`的创建为例子,进行说明

```py
# conf.py
var = {
    'connections': [
        {
            'conn_id': 'ssh_my_own_1',
            'conn_type': 'ssh',
            'host': '127.0.0.2',
            'port': 22,
            'login': 'root',
            'password': 'pwd',
        },
        {
            'conn_id': 'ssh_my_own_2',
            'conn_type': 'ssh',
            'host': '127.0.0.3',
            'port': 22,
            'login': 'root',
            'password': 'pwd',
        },
        ...
    ]
}
# init_conn_var.py
from airflow import DAG, Connection
from airflow.setting import Session
from airflow.operators.python_operator import PythonOperator

def crt_airflow_conn(conf):
    conn = Connection()
    conn.conn_id = conf.get('conn_id')
    conn.conn_type = conf.get('conn_type')
    conn.host = conf.get('host')
    conn.port = conf.get('port')
    conn.login = conf.get('login')
    conn.password = conf.get('password')
    conn.schema = conf.get('schema')
    conn.extra = conf.get('extra')

    session = Session()
    try:
        exists_conn = session.query(Connection.conn_id == conn.conn_id).one()
    except exc.NoResultFound:
        logging.info('connection not exists, will create it.')
    else:
        logging.info('connection exists, will delete it before create.')
        session.delete(exists_conn)
    finally:
        session.add(conn)
        session.commit()
    session.close()

dag = DAG(
    dag_id='create_conn',
    schedule_interval='@once',
)

for connection in conf.get('connection'):
    crt_conn = PythonOperator(
        task_id='create_conn_{}'.format(connection.get('conn_id')),
        pyhton_callable=crt_airflow_conn,
        op_kwargs={'conf': connection},
        provide_context=False,
        dag=dag,
    )
```

### 用户创建

* 通过Airflow UI页面进行创建`Airflow -> Admin -> User`,目前创建补支持密码
* 使用cli命令行进行用户创建`airflow create_user -r <ROLE> -u <USERNAME> -e <EMAIL> -p <PASSWORD>`
* 通过自定义脚本实现,将如下脚本放到`AIRFLOW_HOME`中,当需要创建用户的时候可以运行脚本进行交互式的创建

  ```py
  import getpass

  import airflow
  from airflow import models, settings
  from airflow.contrib.auth.backends.password_auth import PasswordUser

  IS_CORRECT = "Y"
  HINT_THIS_SCRIPT = "\nhint!!\n==> YOU RUN THIS SCRIPT TO CREATE AIRFLOW USER NOW\n"
  HINT_USER = "Please enter username you want to create: "
  HINT_EMAIL_WITH_USER = "Please enter email for user `{username}`: "
  HINT_PASSWORD_WITH_USER = "Please enter password for user `{username}`: "
  HINT_CONFIRM_USER_PASSWORD = "\nhint!! > you want to add user `{username}` with email `{email}`\n" \
      "enter 'Y/y' to confirm the information\nor enter other key to reinput information\n>> "

  user = PasswordUser(models.User())

  while True:
      print(HINT_THIS_SCRIPT)
      user.username = input(HINT_USER)
      user.email = input(HINT_EMAIL_WITH_USER.format(username=user.username))
      user.password = getpass.getpass(HINT_PASSWORD_WITH_USER.format(username=user.username))
      correct = input(HINT_CONFIRM_USER_PASSWORD.format(username=user.username, email=user.email))

      if correct.strip().upper() == IS_CORRECT:
          break

  session = settings.Session()
  session.add(user)
  session.commit()
  session.close()
  ```

## DAG开发流程

### 调试

* airflow创建及调试的顺序
  * 上传/更新DAG文件
  * 检测语法有没有错误`python <文件名>`
  * 使用airflow test运行单个task`airflow test DAG_ID TASK_ID execute_date`

## Airflow operator

这里记录几个Airflow里面常用但是比较难理解的operator

* `BranchPythonOperator`: 通过不同的情况运行对应的下游task.通过`python_callable`参数的返回值确定下游要运行的task,返回值的名称就是要运行task的task_id

## docker-airflow

**update at 2019-04-05**: 已经在我的仓库中创建了[zhongjiajie/docker-airflow][22]定期将[puckel/docker-airflow][7]中优秀的PR合并到master,上面还有一套我自己使用的环境[branch-custom][23]

较常用的镜像是[puckel/docker-airflow][7],这个镜像维护人的热度不高,且[airflow官方的docker][8]进行进行,后期可能会不使用这个版本.下面说明他可能存在的问题

* 将数据库从postgresql切换到mysql:按照airflow官网的方式直接增加`AIRFLOW__CORE__SQL_ALCHEMY_CONN`变量没有效果,因为这个repo的`scripts/entrypoint.sh`有一句`AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB"`指定了数据库的类型和链接信息,即使你在docker-compose指定了`AIRFLOW__CORE__SQL_ALCHEMY_CONN`也会被`scripts/entrypoint.sh`覆盖掉,目前比较可取的方法灵感来自是[这个issue的答案][10],将`AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB"`改成`: "${AIRFLOW__CORE__SQL_ALCHEMY_CONN:="postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB"}"`

## FAQ

* 自定义operator后发现不能运行但是代码没有问题,有可能是自定义operator中的参数和Airflow内部变量的参数同名,如同[ariflow date error][2]中的原因,就是因为子自定义的Operator中定义了一个`start_date`变量,并把变量声明成`template_fields`导致的错误
* airflow schedule_interval设置了`@once`之后dag一直hung,是因为`airflow.cfg`中的`catchup_by_default`设为了True,或者DAG默认参数设为了True,解决上面的问题,只要设置会正确值重启就可.[has usage of @once for scheduler interval changed in v1.9][6]

## Ref

* [解密 Airbnb 的数据流编程神器：Airflow 中的技巧和陷阱][3]
* [DockOne微信分享（一四七）：瓜子云的任务调度系统][4]: 将airflow迁移到k8s上面的思路及主要解决的问题.借助Kubernetes的自动扩展，集群资源统一管理
* [DockOne微信分享（一四七）：瓜子云的任务调度系统][4]
* 非常有用,[Get started developing workflows with Apache Airflow][5]: install, start, DAG, first Operator, plugin, Debugg Operator, **airflow debug with Ipython.embed**, airflow debug with Pycharm, Sensor and it poke function
* [如何部署一个健壮的 apache-airflow 调度系统][19]
* [阿里基于Airflow开发的调度系统maat][20]
* [has usage of @once for scheduler interval changed in v1.9][6]
* [puckel/docker-airflow][7]
* [airflow-pr-Add official dockerfile][8]
* [puckel/docker-airflow issue Example utilizing RDS or other external Postgres instance][9]
* [puckel/docker-airflow issue non-postgres result_backed is overridden by hardcoded values in entrypoint.sh][10]

[1]: https://github.com/zhongjiajie/zhongjiajie.github.com/wiki/Airflow-user
[2]: https://stackoverflow.com/questions/52176131/airflow-date-error-dag-normalize-schedule-typeerror
[3]: https://segmentfault.com/a/1190000005078547
[4]: http://dockone.io/article/2845
[5]: http://michal.karzynski.pl/blog/2017/03/19/developing-workflows-with-apache-airflow/
[6]: https://stackoverflow.com/a/48752253/7152658
[7]: https://github.com/puckel/docker-airflow
[8]: https://github.com/apache/airflow/pull/4483
[9]: https://github.com/puckel/docker-airflow/issues/149
[10]: https://github.com/puckel/docker-airflow/issues/286#issuecomment-450521949
[11]: https://github.com/puckel/docker-airflow/issues/301#issuecomment-457076168
[12]: https://github.com/puckel/docker-airflow/issues/301#issuecomment-457058866
[17]: https://github.com/apache/airflow/pull/4747#issuecomment-465920636
[18]: https://github.com/puckel/docker-airflow/issues/323
[19]: https://www.jianshu.com/p/2ecef979c606
[20]: https://102.alibaba.com/detail?id=172
[21]: https://github.com/apache/airflow/pull/4779#issuecomment-473930751
[22]: https://github.com/zhongjiajie/docker-airflow
[23]: https://github.com/zhongjiajie/docker-airflow/tree/custom
[24]: https://airflow.apache.org/code.html#default-variables
