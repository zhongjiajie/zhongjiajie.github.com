<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2019年度总结-在路上]]></title>
    <url>%2F2020%2F02%2F01%2F2019%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93-%E5%9C%A8%E8%B7%AF%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[2019年度总结 - 在路上今天是2020年2月1日，近两天结束了新春的事务(陪嗑瓜子陪聊人生以及修电脑)，原本回穗的计划因为武汉疫情打断。回望春节放假前制定的春节学习计划又泡汤了，赶紧在趁结束把年度总结写了。 2019年回顾2019年算是体验了半个创业人的辛酸为什么说半个创业人?按照道理我应该算公司的创始人之一，但是和之前工作的感觉也没有太多的差别，春节前我也思考了很久为什么会出现这样的情况，个人总结的原因是–公司决策的参与程度不高。可能是因为公司是直接在客户现场办公，所以一般重要的计划都是跟着客户而变化的原因。也可能是我占的股份不多，导致其实公司开了很多重要的会议，但是没有通知我参加。反正在我看来就是参与公司决策不多，但是我在创业这件事却是事实，所以我只能算是：半个创业人 有一定的成长今年的相比18年在宏观的方面有了较大的进步，之前的我几乎只关注自己的一亩三分地，19年我更加多的从整个项目的角度，这个团队的家角度，整个业务的角度思考问题。之前的我仅关心自己的sql，数据流程使用按时、正确地得到结果。19年的我 在完成自己部分任务的同时，还会关注同事的任务进度，在工作中遇到的问题，看看更加有经验的同事是怎么确定里程碑事件的 除了直接纯粹的写sql的工作中抽厘出来，结合github开源的程序写了数个udf（身份证是否有效 确定生日 性别 行政区（省市））将hive中map类型和字符串间转换 判断电话号码是否有效 开始写后端的业务代码，基于同事代码结构了基础上，写了部分后端的spring boot的业务代码，并重新规划了测试服务和线上服务的配置及部署区别 开始接触较大型的开源代码贡献，向 Apache-airflow 开始贡献代码，主要是一些使用中遇到的小bug，以及希望有的小 feature，并成功冲到了 contributors 的第23名 较为主动的补齐了团队中的短板，发现了公司应用部署还是纯手工的部署，主动承担起编写自动部署脚本的责任。新的团队成员不懂具体的业务逻辑，我会主动的和他解释其中的业务。部分同事之前使用svn，没有接触过git，我为其讲解git的分区及基本的使用。公司需要写对外宣传的PPT，我作为团队中比较熟悉业务和数据的人员，主动承担其了PPT主要的目标部分和业务部分的编写。接到客户新的业务需求，我会主动去和客户进行业务的交流和需求的确认。 基于之前和客户沟通的业务，我更加懂得在业务的角度思考问题，在和客户讨论新需求的时候，我会站在业务+技术的角色给出必要的建议。这样不仅能够更加明确的确定需求，还能给客户的主观感觉上团队的响应和业务理解更加深刻 懂得跳出客户的思维思考问题。我是客户需求为主导的公司，之前的印象就是客户需要什么我们提供什么就完事了，但是19年踩了不少坑，发现并不是这么回事。所以19年我逐渐尝试跳出客户的思维思考问题，这里的跳出并不是说不完成客户的需求，而是在他给我们的需求上增加一些可以增加的东西。因为客户给的需求仅仅是想要看看目前的形式下，这样的决策是否有效，这个功能是否能覆盖业务需求。但是既然客户提出了这个想法，他当然是觉得这个需求应该是可以落地的，如果此时做出来的东西不满足客户原先的设想，我会主动去寻找其中的原因，可能是少考虑了部分的条件，可能是参数的设置有问题的，导致最后的结果和预设值背离。 有一定的不足 主要的工作还是以写sql为主，对Python以及Java的工程应用方面还是比较薄弱 从开始贡献开源代码到现在已经快要一年时间，还没贡献太多的核心代码，主要还是在修复已有的bug或者增加部分小的feature，Aiflow有很多AIP提供给开发者提交patch，我不仅没有没有提交对应的代码，甚至很多AIP我都没有去了解 从部署自动化开始使用较多的shell脚本，但是现在还有很多函数是使用过来就忘记，没有很多的记录和复习方案去专门克服这方面的内容 由于直接在客户现场办公，很多的会议加上很多的临时需求时常会导致原先设定的周计划泡汤，导致每天基本上是加班的状态，想要晚上补齐之前欠下的计算机基础课程都成了一个难题 沟通能力有待加强，很多时候我能理解用户的意思，但是表达的时候总会言不达义，导致沟通成本上有所增加，能意简言赅地表达出想要表达的内容是今年的一个目标 2019年定下的目标及完成的情况回顾一下19年定下的目标 技术展望 python能从小学生到中学生过度：这个算是成功了一半吧，毕竟我觉得我现在也没能太好的掌握 java能从新手到小学生过度：真的只是小学生，hello world 级别 希望增强基础理论的学习，如数据结构和基本算法：失败，大大的失败，非常失败 加强hadoop家族尤其是spark的学习，跟上时代的步伐：失败，大大的失败，非常失败 静下心看1-2本进阶的书籍，摆脱小学生的能力：失败，大大的失败，非常失败 能热情得投入开源项目的怀抱，积极参加一个较大的开源项目：算成功吧 主要贡献了Apache-airflow，以及aiflow相关的如Aapche-airflow-中文文档 datax自己维护了一个版本fork：zhongjiajie/datax patch了几个EbookFoundation/free-programming-books 能好好通过wiki积累知识，能通过blog和大家分享我学到的东西：还算成功，目前是每一个月更新一次zhongjiajie/wiki 整理好github的repo，将类似的repo合并，将没有意义的repo删除：已经完成了 生活展望 热爱我的生活：这个应该说是失败了的，我的19年生活过得一般，工作日是在加班中度过的，周末是在睡懒觉+洗狗+做饭中度过的，没有太多让我热爱的部分 希望能够乐观地对待这个世界，同时也被这世界善待：19年其实……并没有很乐观，唉，可能我是一个悲观的人吧 坚持锻炼，即使每天30分钟也比没有强：一开始是有的，后面就gg了，没有坚持起来，所以这个也是20年的目标 周末不要睡太晚，早点起床即使不学习也可以运运动，买买菜：oh，shit，恩，这个也会是20年的目标 晚上早点睡觉，争取12点前睡着：同上，真是，生活习惯不怎么好呀 不求生活给我惊喜，只求它别给我太多惊吓：确实没有惊喜，同时惊吓也是一半而已，可以接受 理财展望 输少当赢：在20年春节前我终于实现了基金翻红。本来应该是20年春节前有较大的盈利的，覆盘的时候发现重要的原因是我在整个19年市场最低的时候，没有多于的资本去加仓，导致整体的持有成本非常高，这个给小伙伴们一个建议，如果想要买基金的话（定投），一定要将资金分配好，不要前期过量的投入，等到市场低点的时候发现没有资本加仓。同时，自己一定要做好现金流的估计，不要将自己挤到进退两难的位置 19年年末有存款：虽然仅有一点点，但是还算是有存款 情感展望 希望情感不会被柴米油盐打败：目前还算成功 带女朋友见家长，希望双方都喜欢对方：只完成了前面的一半，家人对女票的印象一般，但是我会尽我最大的能力让家人接受女票的 2020年目标20年如果全部的目标都能实现的话，将会有*个里程碑事件：跳槽去较大的本地企业、成为Apache-Airflow committer、买房、养成良好的生活习惯并热爱生活 跳槽去较大的本地企业：第一个公司base是杭州的to-B企业，现在的公司是和前同事一同创立的企业。第一个公司由于支持不足，要撤离在广州的研发人员，所以和前同事创立了现公司，但是由于规模太小，资金回笼过慢暴露了很多问题，我个人已经决定了将要退出公司去别的公司求职，目前希望公司是base广州的或者广州有较大研发团队的，20年遇到国内疫情，可能HC的数量会少一些，但是也希望可以找到 成为Apache-Airflow committer：目前已经贡献了23个commit，虽然说有部分是非常小的贡献，显示已经是贡献榜的第23名了，20年的目标是成为committer。也不是说贪慕committer的名头，只是觉得要有一个目标才能更好的努力。我会仔细阅读Airflow-AIP的部分（目前还没有仔细的了解全部的AIP内容），了解下Roadmap，更多的参与社区的讨论和 pull request 的 review 工作，争取提升自己整体认识的同时成为一个得到社区认可的人 买房：安居才能乐业，我觉得也差不多考虑这个问题的时候了，当然可能资金不够会承压，但是我还是愿意去尝试一下 养成良好的生活习惯并热爱生活：上了大学以及毕业后的生活习惯确实不怎么好，没有很好的形体、没有日常锻炼、没有早睡早起、没有什么仪式感、不够乐观地看待生活、看待身边的人和事，我认为这是影响人一生的东西，之前没有养成良好的习惯，但是我愿意在我还年轻的时候纠正它，让它朝着美好的方向发展 工作展望 提高英文读写能力 跳槽去base本地的大厂 成为Apache-Airflow committer HDFS Hive Spark 了解大概并找一个深入研究 看完&lt;流程的Python&gt; 挑选着看&lt;Java核心技术卷&gt; 增加计算机基础课程的补习 生活展望 抬头挺胸 坚持锻炼 早睡早起 情感不会被柴米油盐打败 热爱生活本身 过有意义的周末(制定有意义的周末计划) 存款到达500k-550k 买房 总价300W或以上 拔两个智慧齿 乐观地对待世界 被这世界善待 理财展望 基金总体能获利10%-20%(在上证3000-3300之间运行 在第一季度开始逐渐抛售 第二季度开始留意政治影响) 信用卡固额提到230k]]></content>
      <categories>
        <category>年度总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Airflow-钉钉消息dingdingOperator使用]]></title>
    <url>%2F2019%2F07%2F05%2FAirflow-%E9%92%89%E9%92%89%E6%B6%88%E6%81%AFdingdingOperator%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Airflow 发送钉钉消息的 dingdingOperator 已经随着 Airflow 1.10.3 一起发布了, 有了 dingdingOperator 我们可以在 Airflow 中更优雅地发送钉钉消息,以及 任务 失败 成功 重试 sla过时等的通知. 背景在较久之前,Airflow对消息系统的支持仅仅是email以及slack.国内习惯使用IM系统作为通知,email一般比较少.slack在国内的普及程度远远没有达到钉钉和微信的水平,所以就有了在Airflow中新加钉钉或者微信作为消息通知的功能 由于我司使用钉钉作为内部通信而非企业微信(好像只有企业微信才开放了webhook?),所以我只给Airflow增加了钉钉的 operator,相关PR在AIRFLOW-1526 在没有Airflow dingdingOperator之前在dingdingOperator没有合并到Airflow master分支之前,网上已经有方案解决Airflow使用钉钉发送消息通知问题 Apache-airflow 钉钉机器人插件: 需要入侵Airflow发送email的源码,需要对Airflow的basemodel类(一个非常基础的类)进行修改,增加ding_on_failure以及ding_on_retry,并重新打包代码 有没有不对源码修改并且能实现发送系统消息的功能呢?答案是肯定的,可以使用Airflow中各种已经定义好了的callback.dingdingOperator正是通过这个方法实现了消息通知的功能的 dingdingOperator实现原理dingdingOperator的实现原理参考了slack_wehook的实现.主要的逻辑都在DingdingHook这个类中,通过继承HttpHook,将用户传过来的消息通过钉钉的webhook进行发送 dingdingOperator怎么使用详情参考这里,已经默认定义了一个dingding_default作为默认的connection,只需要修改webhook详情就可以了,注意: 需要将webhook放入password字段中, 且仅仅需要token而不是一整串的webhook 一般的使用方法为 1234567891011text_msg_remind_all = DingdingOperator( task_id='text_msg_remind_all', dingding_conn_id='dingding_default', message_type='text', message='Airflow dingding text message remind all users in group', # list of user phone/email here in the group # when at_all is specific will cover at_mobiles at_mobiles=['156XXXXXXXX', '130XXXXXXXX'], at_all=True, dag=dag,) 如果想要发送富文本内容 123456789101112131415markdown_msg = DingdingOperator( task_id='markdown_msg', dingding_conn_id='dingding_default', message_type='markdown', message=&#123; 'title': 'Airflow dingding markdown message', 'text': '# Markdown message title\n' 'content content .. \n' '### sub-title\n' '![logo](http://airflow.apache.org/_images/pin_large.png)' &#125;, at_mobiles=['156XXXXXXXX'], at_all=False, dag=dag,) 支持的消息类型目前支持 普通消息, link, markdown, actionCard 和 feedCard, 考虑到用户对普通消息的需求更大,所以将普通消息进行了封装,不必传content关键字,只需要传消息的内容就行 通过dingdingoperator发送DAG状态的消息上面的方法是使用dingdingOperator发送消息,本质是定义了一个dingdingOperator实例化后的task,放在DAG中的某个位置,当上游完全满足条件的时候由scheduler触发这个task. 但是作为消息通知,更常使用的场景是: 当Airflow中DAG运行到某个状态(task成功 失败 重试等)的时候发送消息通知对应的用户,这种情况就类似与callback函数,它应该作为DAG内部的一部分,而是不是仅仅是一个task. 这种情况下我们可以使用Task callback,在定义DAG的时候将DingdingOperator传到DAG.default_args的属性中,支持的callback类型包括sla_miss_callback, on_success_callback, on_failure_callback, 或者 on_retry_callback, 下面我们以 on_failure_callback 为例 12345678910111213141516171819202122232425262728293031args = &#123; 'owner': 'airflow', 'retries': 3, 'start_date': airflow.utils.dates.days_ago(2),&#125;def failure_callback(context): message = 'AIRFLOW TASK FAILURE TIPS:\n' \ 'DAG: &#123;&#125;\n' \ 'TASKS: &#123;&#125;\n' \ 'Reason: &#123;&#125;\n' \ .format(context['task_instance'].dag_id, context['task_instance'].task_id, context['exception']) return DingdingOperator( task_id='dingding_success_callback', dingding_conn_id='dingding_default', message_type='text', message=message, at_all=True, ).execute(context)args['on_failure_callback'] = failure_callbackdag = DAG( dag_id='example_dingding_operator', default_args=args, schedule_interval='@once', dagrun_timeout=timedelta(minutes=60),) 如上面的例子,当DAG中有task报错时,会触发dingding_success_callback这个task,发送一个钉钉的消息到群中,并且@群里的全部人员.message中对应的信息是通过context获取的 如果消息中定义更多的类型,可以自行定义,context对象支持的属性有 get_template_context返回的全部类型, 其中有几个比较大的对象是: task instance对象,task_instance有的属性 configuration对象,configuration有的属性 数据血缘关系inlets和outlets 如果任务失败会还会增加exception属性 以上,希望大家可以更加方便的使用dingding发送系统消息.]]></content>
      <categories>
        <category>Airflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Airflow 用户指南]]></title>
    <url>%2F2019%2F04%2F05%2FAirflow-%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Airflow 用户指南Airflow用户指南,基于我的wiki提炼出来的内容,简单描述了使用Airflow过程中有什么注意事项及需要关注的点 依赖关系根据committer成员的讨论,更加推荐使用位操作符(bitwise operators)来解决依赖 最原始的方式task.set_upstream(task1); task.set_downstream(task2) 位操作(bitwise operators)方式,airflow1.8之后task &gt;&gt; task1; task &lt;&lt; task1; task &gt;&gt; task1 &lt;&lt; task2 DAG的位操作符号可以提供更多的功能: t1 &gt;&gt; [t2, t3] &gt;&gt; t4已经能被支持了 使用chain方法实现多个task的依次依赖(Airflow 1.10.3已经取消chain方法) chain的一般使用 12from airflow.utils.helpers import chainchain(task, task1, task2) 通过列表解析直接生成task列表然后chain起来 123from airflow.utils.helpers import chainds_true = [DummyOperator(task_id='true_' + str(i), dag=dag) for i in [1, 2]]chain(cond_true, *ds_true) 一对多的链接关系 t1 &gt;&gt; [t2, t3](推荐) group = [task1, task2, task3]; task.set_downstream(group); 多对一的链接关系 [t1, t2] &gt;&gt; t3(推荐) group = [task1, task2, task3]; task.set_upstream(group) 多对多的笛卡尔积 airflow.utils.helper.cross_downstream([t1, t2, t3], [t4, t5, t6])(推荐) 使用自定义的方法 123456import itertoolsimport airflow.utils.helper.chaingroup_a=[task1, task2, task3]group_b=[task4, task5]for pair in itertools.product(group_a, group_b): chain(*pair) 如果有根据一定条件选择下游执行哪个task操作的逻辑,可以使用BranchPythonOperator算子,使用是可以通过TriggerRule.ONE_SUCCESS设置实现.例如例子A是BranchPythonOperator,一个分支运行B,另一个分支运行C,同时B-&gt;C,这时可以在C中设置TriggerRule.ONE_SUCCESS.以前我总认为一个该这样实现,会多个两个算子 时间相关 使用Airflow内置的日期宏 airflow内置了部分时间相关的参数,如 '{{ ds }}' 代表运行的时间, '{{ yesterday_ds }}' 运行时间昨天的日期,更多时间相关的参数见这里 自定义时间参数 简单的时间操作: 通过replace完成,获取运行日期同时改变成特殊的时间 some_command.sh {{ execution_date.replace(day=1) }} 通过macros对时间进行更多操作: macros.ds_add将内置时间进行计算 '{{ macros.ds_add(ds, 1) }}' 资源限制作为一个工作流工具,除了完成各种复杂的上下游关系外,我认为解决资源的限制也是很重要的点.资源限制包括限制DAG的并发,限制多个DAG的运行关系.限制同一个/同一类Task的并发 DAG的限制 限制dag并行实例数量 在airflow.cfg的[core]设置dag_concurrency限制并行数量 在DAG文件中限制 1234567891011121314from airflow import DAGdefault_args = &#123; 'owner': 'airflow', # here to set value 'concurrency': 10&#125;dag = DAG( 'tutorial', default_args=default_args, description='A simple tutorial DAG', schedule_interval=timedelta(days=1),) Task的限制 限制task的并行数量: operetor中的参数task_concurrency可以设置task的并行数量 限制多个不同类型的task并行数量: operetor中的参数pool限制一类task的并行数量,与task_concurrency参数的区别是task_concurrency设置的是同一个task的并行数task_id要相同,pool设置的是一类task的并行数task_id可以不同,只要保证pool参数的名称相同就可以.设置后并行的task不会超过pool对象的slots值 将Airflow中的对象通过DAG或者脚本的方式进行保存connection variables pool目前Airflow创建connection variables pool能通过如下方式创建: Airflow的cli命令创建,对应命令分别为: airflow connections --add airflow variables -s airflow pools -s Airflow的web UI页面进行设置,分别为: Admin -&gt; connections Admin -&gt; variables Admin -&gt; pools 这里提供一个将connection variables pool固定到DAG的方法,查看这里.主要是之前使用docker-airflow每次重启时都会清空postgre数据,这样能保证connection variables pool能被git进行版本管理,下面以connections的创建为例子,进行说明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# conf.pyvar = &#123; 'connections': [ &#123; 'conn_id': 'ssh_my_own_1', 'conn_type': 'ssh', 'host': '127.0.0.2', 'port': 22, 'login': 'root', 'password': 'pwd', &#125;, &#123; 'conn_id': 'ssh_my_own_2', 'conn_type': 'ssh', 'host': '127.0.0.3', 'port': 22, 'login': 'root', 'password': 'pwd', &#125;, ... ]&#125;# init_conn_var.pyfrom airflow import DAG, Connectionfrom airflow.setting import Sessionfrom airflow.operators.python_operator import PythonOperatordef crt_airflow_conn(conf): conn = Connection() conn.conn_id = conf.get('conn_id') conn.conn_type = conf.get('conn_type') conn.host = conf.get('host') conn.port = conf.get('port') conn.login = conf.get('login') conn.password = conf.get('password') conn.schema = conf.get('schema') conn.extra = conf.get('extra') session = Session() try: exists_conn = session.query(Connection.conn_id == conn.conn_id).one() except exc.NoResultFound: logging.info('connection not exists, will create it.') else: logging.info('connection exists, will delete it before create.') session.delete(exists_conn) finally: session.add(conn) session.commit() session.close()dag = DAG( dag_id='create_conn', schedule_interval='@once',)for connection in conf.get('connection'): crt_conn = PythonOperator( task_id='create_conn_&#123;&#125;'.format(connection.get('conn_id')), pyhton_callable=crt_airflow_conn, op_kwargs=&#123;'conf': connection&#125;, provide_context=False, dag=dag, ) 用户创建 通过Airflow UI页面进行创建Airflow -&gt; Admin -&gt; User,目前创建补支持密码 使用cli命令行进行用户创建airflow create_user -r &lt;ROLE&gt; -u &lt;USERNAME&gt; -e &lt;EMAIL&gt; -p &lt;PASSWORD&gt; 通过自定义脚本实现,将如下脚本放到AIRFLOW_HOME中,当需要创建用户的时候可以运行脚本进行交互式的创建 123456789101112131415161718192021222324252627282930import getpassimport airflowfrom airflow import models, settingsfrom airflow.contrib.auth.backends.password_auth import PasswordUserIS_CORRECT = "Y"HINT_THIS_SCRIPT = "\nhint!!\n==&gt; YOU RUN THIS SCRIPT TO CREATE AIRFLOW USER NOW\n"HINT_USER = "Please enter username you want to create: "HINT_EMAIL_WITH_USER = "Please enter email for user `&#123;username&#125;`: "HINT_PASSWORD_WITH_USER = "Please enter password for user `&#123;username&#125;`: "HINT_CONFIRM_USER_PASSWORD = "\nhint!! &gt; you want to add user `&#123;username&#125;` with email `&#123;email&#125;`\n" \ "enter 'Y/y' to confirm the information\nor enter other key to reinput information\n&gt;&gt; "user = PasswordUser(models.User())while True: print(HINT_THIS_SCRIPT) user.username = input(HINT_USER) user.email = input(HINT_EMAIL_WITH_USER.format(username=user.username)) user.password = getpass.getpass(HINT_PASSWORD_WITH_USER.format(username=user.username)) correct = input(HINT_CONFIRM_USER_PASSWORD.format(username=user.username, email=user.email)) if correct.strip().upper() == IS_CORRECT: breaksession = settings.Session()session.add(user)session.commit()session.close() DAG开发流程调试 airflow创建及调试的顺序 上传/更新DAG文件 检测语法有没有错误python &lt;文件名&gt; 使用airflow test运行单个taskairflow test DAG_ID TASK_ID execute_date Airflow operator这里记录几个Airflow里面常用但是比较难理解的operator BranchPythonOperator: 通过不同的情况运行对应的下游task.通过python_callable参数的返回值确定下游要运行的task,返回值的名称就是要运行task的task_id docker-airflowupdate at 2019-04-05: 已经在我的仓库中创建了zhongjiajie/docker-airflow定期将puckel/docker-airflow中优秀的PR合并到master,上面还有一套我自己使用的环境branch-custom 较常用的镜像是puckel/docker-airflow,这个镜像维护人的热度不高,且airflow官方的docker进行进行,后期可能会不使用这个版本.下面说明他可能存在的问题 将数据库从postgresql切换到mysql:按照airflow官网的方式直接增加AIRFLOW__CORE__SQL_ALCHEMY_CONN变量没有效果,因为这个repo的scripts/entrypoint.sh有一句AIRFLOW__CORE__SQL_ALCHEMY_CONN=&quot;postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB&quot;指定了数据库的类型和链接信息,即使你在docker-compose指定了AIRFLOW__CORE__SQL_ALCHEMY_CONN也会被scripts/entrypoint.sh覆盖掉,目前比较可取的方法灵感来自是这个issue的答案,将AIRFLOW__CORE__SQL_ALCHEMY_CONN=&quot;postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB&quot;改成: &quot;${AIRFLOW__CORE__SQL_ALCHEMY_CONN:=&quot;postgresql+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB&quot;}&quot; FAQ 自定义operator后发现不能运行但是代码没有问题,有可能是自定义operator中的参数和Airflow内部变量的参数同名,如同ariflow date error中的原因,就是因为子自定义的Operator中定义了一个start_date变量,并把变量声明成template_fields导致的错误 airflow schedule_interval设置了@once之后dag一直hung,是因为airflow.cfg中的catchup_by_default设为了True,或者DAG默认参数设为了True,解决上面的问题,只要设置会正确值重启就可.has usage of @once for scheduler interval changed in v1.9 Ref 解密 Airbnb 的数据流编程神器：Airflow 中的技巧和陷阱 DockOne微信分享（一四七）：瓜子云的任务调度系统: 将airflow迁移到k8s上面的思路及主要解决的问题.借助Kubernetes的自动扩展，集群资源统一管理 DockOne微信分享（一四七）：瓜子云的任务调度系统 非常有用,Get started developing workflows with Apache Airflow: install, start, DAG, first Operator, plugin, Debugg Operator, airflow debug with Ipython.embed, airflow debug with Pycharm, Sensor and it poke function 如何部署一个健壮的 apache-airflow 调度系统 阿里基于Airflow开发的调度系统maat has usage of @once for scheduler interval changed in v1.9 puckel/docker-airflow airflow-pr-Add official dockerfile puckel/docker-airflow issue Example utilizing RDS or other external Postgres instance puckel/docker-airflow issue non-postgres result_backed is overridden by hardcoded values in entrypoint.sh]]></content>
      <categories>
        <category>Airflow</category>
      </categories>
      <tags>
        <tag>Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何加入Apache Airflow slack中国用户频道]]></title>
    <url>%2F2019%2F04%2F04%2F%E5%A6%82%E4%BD%95%E5%8A%A0%E5%85%A5Apache-Airflow-slack%E4%B8%AD%E5%9B%BD%E7%94%A8%E6%88%B7%E9%A2%91%E9%81%93%2F</url>
    <content type="text"><![CDATA[这篇文章是介绍如何加入Airflow slack中国用户频道#user-china 为什么要加入Airflow slack user-chinaApache Airflow 官方的 chat channel 是 slack,里面除了有很多的使用者和代码贡献者外,还有 committer 和 PMC member.在上面问问题能够得到较快且交正确的解答. 由于 Airflow 在中国不算太流行,所以在看这篇教程之前你可能已经加入了多个微信/QQ群,可能你会问:为什么我还要加入 slack?这个软件我没有用过而且是英文的,我在微信/QQ群已经能得到解答了.我个人认为原因如下: 官方谈论频道:个人认为这个非常重要,因为Airflow在国内使用并不多,有官方的支持才能保证社区的活跃且不关闭 讨论更为集中:目前太多的微信/QQ群,导致同一个问题可能在不同的地方被提问,如果集中在一个地方讨论会更加集中,有利于问题的解决 更多参与者:目前微信/QQ群都是国内的用户,但是在 slack 中可以看看国外大神是怎么使用 Airflow 的,遇到问题是怎么解决的.很多人会从源码层面解释这个问题的原因 创建了中文频道:由于以上原因,我向PMC成员ashb申请了Airflow中文频道,使用 想要二次开发 或者想要向Airflow共享代码的各位可以加入slack的讨论 加入的步骤 浏览器输入https://apache-airflow-slack.herokuapp.com/,跳转到join页面,填入邮箱 点击join,获得提示 登录刚刚填写的邮箱点击join now(可能邮件在垃圾箱) 填写必要信息 下一步并同意 点击channel搜索 加入users-china]]></content>
      <categories>
        <category>Airflow</category>
      </categories>
      <tags>
        <tag>Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA['工资突然崩塌式下降是什么感受'有感]]></title>
    <url>%2F2019%2F01%2F17%2F%E5%B7%A5%E8%B5%84%E7%AA%81%E7%84%B6%E5%B4%A9%E5%A1%8C%E5%BC%8F%E4%B8%8B%E9%99%8D%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%9F%E5%8F%97-%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[今天公交上读到工资突然崩塌式下降是什么感受？ - 知乎突然内心涌起波澜.答主毕业时进入与专业对口,大家都羡慕的高薪行业,最要的是如果一直干可以到退休(如果我有这样的工作一定不会放弃,毕竟码农30岁是一个大坎).因为爱情选择了去女友的大城市发展,一开始工资有了提升,但是行业温水主青蛙式崩塌答主全然不知,最后被辞退,歇业了一段时间,当再次找到工作时工资下跌一半 当然,让我感到最沉重的莫过于答主引用陶杰先生的一句话: 当你老了，回顾一生，就会发觉：什么时候出国读书，什么时候决定做第一份职业、何时选定了对象而恋爱、什么时候结婚，其实都是命运的巨变。只是当时站在三岔路口，眼见风云千樯，你作出选择的那一日，在日记上，相当沉闷和平凡，当时还以为是生命中普通的一天。 – 陶杰 《杀鹌鹑的少女》 看了看评论,有一句话也非常值得我们去思考: 可能是缺乏战略眼光导致选择失误，最开始年轻选择多容易忽略，尤其是执行力可塑性精明程度都强的情况下，容易自己把棋走死。即使没有跳槽也可能面临类似结局吧。心智模式才是决定因素，所谓行业都是在不断变化，做不得数。 – David han 我最近总在思考一个问题,我30岁后能干什么?我是继续在办公室吹着暖气写着代码,还是在界面吹着冷风卖这煎饼?大三的我,听了几场师兄师姐的建议,决定了以后从事计算机工作,当时我是一个数学系学生.大三老师推荐了一份校企合作的企业,开始了计算机的打怪升级之路,怪打了不少,但是升级没有多少.大四我自己找了份互联网+政务的企业,这个行业有一个特点,就是技术和业务都不是最重要的,最重要的是人脉,有人,就没有做不成的事.可偏偏我就是做技术的(各位别笑,码农也是技术),更为糟糕的是,生活中一些变化包括辞职,理财失败,给了家人一笔现金等,导致我现金流紧缺.现在正和前同事创业,给的工资比之前高了一点,但是不确定性却高了数倍.加入创业时我认为:没关系,我还年轻,很多东西没有就没有吧.第一次我去介意这种事情的时候是在我去医院看病,听说定点可以增加报销额度,我就定点,工作人员和我说,我的社保卡是封存状态不能定点,才想起我已经小半年没有较社保公积金了,如果此时我有什么大病,后果将是不堪设想的,现金流弱+没有医保.(这里一个小插曲,我离职前买了意外险,定寿,重疾,就是没有买医疗险,当我意识到医疗问题的严重性后,想加一款医疗险发现很多都要有社保缴费记录,没有缴费记录的都死贵死贵) 我就在想,等我30岁,或者更年长的时候看这几个决定,我是什么心态看的?那时的我会悔恨现在的我么?我要怎么做才能减少未来的我对现在的我的悔恨?]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年度总结]]></title>
    <url>%2F2019%2F01%2F11%2F2018%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[2018年度总结-我还是小学生今天是2019年1月11日,这是第一次在github page写年度总结,希望这不是最后一次,希望我能坚持下来.2018年是我毕业一来最为复杂的一年,也是最难的一年.好吧一共毕业才1年半,也验证了成年人的世界里没有容易这句话. 2018年历险记2018年过完春节,我们搬了家,搬家真是各劳神伤财的活.同时因为公司在长沙拿了一个大项目,同时广州的项目接连没有得到用户的认可,我们base广州的几个人被建议去长沙或者回公司base杭州发展.基于很多原因考虑,我和同时都选择了长沙,所以2018年春节可以说是我第一次出省工作的一年. 长沙给我的印象是风景很美,人很豪爽,交通相对流畅,还有很辣.我在长沙吃了我人生中第一次放了辣椒的西红柿炒蛋…换了新的环境,接触的东西也多了很多,由于项目原因开始解除java,基于spring和dubbo完成的开发.说实在话,对于一个从python sql学起的非科班码农,java显得特别不友好,一开始最抵触的一点就是参数和结果类型的转换问题,经常因为不知道返回值什么类型但是idea一直提示错误(这里不得不说idea是写java很好的ide).幸亏同事比较友善,很乐意帮我解决我对java的疑问.那段期间我渐渐对java有了一些皮毛的了解,可以看得懂简单的代码了.写了其中一部分小功能. 同时,项目的测试服务器管理放到了我这边,公司有一台外网的测试服务器和四台内网测试服务器,都是从裸机开始搭建的,外网只是用来对外演示用,所以大部分应用,如mysql,zookeeper,rocketmq都是用docker容器+docker-compose部署.期间了写了一键重启docker-compose,一键从gitlab中拉取最新代码更新应用的shell.这大概是我在长沙项目作出的全部贡献了. 长沙的项目做的是应用的中台,目的是实现应用发布平台,下级部门将建设完后,相关人员对应用进行提交,审核,研判,发布工作,后台接口跟踪流程,进度.分析各部门应用的数量,应用使用情况,提供用户对应用评价,建议等功能 7月份事情有了转折,我长期在外出差,女朋友一人和一只狗在广州,加上女朋友刚毕业就遇到37互娱式变态加班,心态有点崩,几次电话都哭泣,我也心感内疚.所以7月就离职走人了.回到广州,想要投简历,遇到之前的同事创业,和之前的工作类似,想了想就去了,从次开启了地狱模式. 工作内容确实类似,都是数据工程,写写sql,规划写数仓数据,但是平台是自己搭的,而且是一个人新搭的.架构从hive+hadoop -&gt; hadoop+yarn+hive -&gt; hadoop+hive+yarn+spark,最后决定了使用spark的STS作为平台的主要入口接收sql,数据同步使用sqoop和datax,平台调度使用airflow.但是spark thrift server确实比hive server弱一点,经常对挂掉,所以平台同事希望将sql转成spark dataframe直接操作HDFS中的数据,然后每个流程用spark-submit提交java类.我肯定是拒绝的,我认为数仓大部分是业务层的东西,理解业务并解决他才是王道,这时候用通用的sql会对实现更加友好;业务层很多东西都是容器改变的,写成java程序打包变更太频繁;数仓的东西高度依赖上下游,如果有业务功能需要在数据流中找一个最优的地方加上业务,如果用spark-submit要去源码中看实现了什么功能然后再加显得十分笨拙;接受部分是spark-submit任务,但是全部切换成spark-submit方式我坚决反对.为此和同事吵了很多次,甚至至今谁都不服谁. 说说目前的项目,项目整体方案使用的是hadoop+yarn+spark+mysql+Elasticsearch+redis+spring boot+react+ArcGIS+echarts这样的解决方案,做的是一个特定行业的内部分析系统,最后给用户使用的是一套数据分析操作,及基础数据结合GIS的展现系统 调度方面开始正式使用airflow,初级是了解airflow是在17年毕业的时候,觉得用代码定义DAG流特别牛叉,平台部署的是使用github上最多start的airflow docker repo,由于业务需要我fork了项目并增加了很多需要的包,制作了自己的镜像部署,部署使用CeleryExecutor+2个worker在单台机器上部署,期间了遇到了很多坑,都爬过来的现在他已经能稳定运行,但是scheduler时间长了会僵死的问题还是没能解决,找遍了社区的方案也只有定时重启这个方法.我个人希望在19年可以对airflow进行较深入的研究,并将我踩过的坑,积累的经验分享到博客. 号外,最近airflow已经成为apache顶级项目了 目前,新公司的项目还在招投标阶段,有中标的希望拿到我在公司的第一个标,同时也是公司的第一个标.但是苦逼的我却要每天熬夜奋战在写投标文件和招标文件. 2018年理财方面可谓一塌糊涂,可谓白打工,最惨的月份甚至是负资产,遥想我毕业的时候还有50K的存款,加上一年半工作的收入.居然会有资产为负数的月份,全靠两张信用卡活到现在.可见2018年花费太多,或者理财过于失败(主要投了个跑路的P2P).2018年让我对现金流水的重要性有了个很好的认识,2019年不求将我18年亏的赚回来,我只求19年不要亏太惨,输少当赢 2019总结起来就是: 第一次长期出差工作经历 第一次写java生产级项目经历(虽然到现在都不知道dubbo是什么个原理,但是还是完成了上级交的工作) 第一次正式工作离职 第一次参加创业 第一次选择和正式环境下使用airflow 第一次亲自参加招标,准备投标 第一次在小规模使用spark 第一次小规模使用elasticsearch 学会了在用户角度考虑产品的发展 python开始写得比小学生高级一点 展望2019年技术展望 python能从小学生到中学生过度 java能从新手到小学生过度 希望增强基础理论的学习,如数据结构和基本算法 加强hadoop家族尤其是spark的学习,跟上时代的步伐 静下心看1-2本进阶的书籍,摆脱小学生的能力 能热情得投入开源项目的怀抱,积极参加一个较大的开源项目 能好好通过wiki积累知识,能通过blog和大家分享我学到的东西 整理好github的repo,将类似的repo合并,将没有意义的repo删除 生活展望 热爱我的生活 希望能够乐观地对待这个世界,同时也被这世界善待 坚持锻炼,即使每天30分钟也比没有强 周末不要睡太晚,早点起床即使不学习也可以运运动,买买菜 晚上早点睡觉,争取12点前睡着 不求生活给我惊喜,只求它别给我太多惊吓 理财展望 输少当赢 19年年末有存款 情感展望 希望情感不会被柴米油盐打败 待女朋友见家长时,希望双方都喜欢对方]]></content>
      <categories>
        <category>年度总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shadowsocks使用技巧]]></title>
    <url>%2F2018%2F04%2F01%2Fshadowsocks%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[Shadowsockstimeline update at 2019-01-11: update content from personal wiki. 记录了安装使用ss过程中使用到的文档,包括后期遇到的问题及相关的解决方案 github-shadowsocks 相关资料 逗逼根据地 Shadowsocks指导篇（总结归类）——从无到有，境无止尽！ Shadowsocks（Sock5代理）的PAC模式与全局模式与VPN的区别 一些便宜性价比高的VPS推荐 wuchong-科学上网之 Shadowsocks 安装及优化加速 全套包括客户端 Github 如何关闭SS的日志: ss的日志问题，日志等级及日志的输出位置 关于PAC的自定义规则 ShadowSocks 自定义规则 Shadowsocks 进阶之 PAC 撰写 Adblock Plus 过滤规则： Adblock Plus官网 user-rule用该规则定义 关于GFWlist github-gfwlist/gfwlist: 一定时间更新 通过base64编码 github-itcook/gfwlist2pac： 时间复杂度O(1)，将base64编码还原成pac文件 用了ss还是上不了或者慢的问题 PAC模式失效是要使用全局模式，定时更新客户端，更新PAC 关于s3下载慢的问题，可以用ss开全局模式下载 s3慢还可以在host文件中增加一条路由219.76.4.4 github-cloud.s3.amazonaws.com FAQ ss开了多个端口且多人使用后，发现 ssseerver 服务经常死掉，参照shadowsocks进程停止后自动重启写了一个定时检查的任务运行 123456#!/bin/shproc_name="shadowsocks"number=`ps -ef | grep $proc_name | grep -v grep | wc -l`if [ $number -eq 0 ]; then ssserver -c /etc/shadowsocks.json -d startfi 将文件放在任意目录下，如 /root 启动系统的 cron 任务 sudo service cron start 编辑 crontab 任务，增加一个任务 注意路径要是全路径 */1 * * * * /bin/bash /path/to/file ，其中 */1 * * * * 代表每分钟运行一次 Ref github-shadowsocks Shadowsocks（Sock5代理）的PAC模式与全局模式与VPN的区别 Shadowsocks指导篇（总结归类）——从无到有，境无止尽！ 一些便宜性价比高的VPS推荐 科学上网之 Shadowsocks 安装及优化加速 如何关闭SS的日志 ShadowSocks 自定义规则 Shadowsocks 进阶之 PAC 撰写 Adblock Plus 过滤规则 github-gfwlist/gfwlist github-itcook/gfwlist2pac shadowsocks进程停止后自动重启]]></content>
      <categories>
        <category>shadowsocks</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>梯子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu常用软件安装和配置]]></title>
    <url>%2F2018%2F04%2F01%2Fubuntu%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ubuntu常用软件安装和配置timeline update at 2019-01-11: ubuntu 18.04 LTS 的常用配置 简介文章介绍了自己第一次接触 Linux 系统的软件安装及常用配置，作为一名应届新萌，一直不能流畅的操作服务器，所以决定逼自己一把，直接将自己的操作系统换成 ubuntu wifi驱动缺失ubuntu18.04默认没有wifi驱动,安装成功后需要将安装介质插入,然后在里边手动安装驱动,安装过程中可能有依赖,建议此时网线联网 ubuntu国内源在清华大学ubuntu镜像帮助页面选择对应的ubuntu版本，然后将系统原来的/etc/apt/sources.list进行备份，sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup，将页面中的配置覆盖原/etc/apt/sources.list文件的内容，这个运行sudo apt-get update &amp;&amp; sudo apt-get upgrade 优化流程的配置sudo免密12sudo vim /etc/sudoersyour_user_name ALL=(ALL) NOPASSWD: ALL gnome扩展打开gnome-extensions在浏览器中添加gnome的插件,然后在terminal中添加gnome-shell 1sudo apt-get install chrome-gnome-shell 之后就可以直接在浏览器中搜索插件并安装了 Pixel Saver: 减少最大化后的窗口标题大小 Dash to Dock: 将会自定折叠dock,调整dock的位置,增加屏幕的大小 应用程序shadowsocks客户端外面的世界很精彩,ss自然也会在我的安装清单中,各种系统的安装请参考页面安装指南. ubuntu推荐用ppa方式安装 ~~12345678910111213141516171819202122~~sudo add-apt-repository ppa:hzwhuang/ss-qt5~~~~sudo apt-get update~~~~sudo apt-get install shadowsocks-qt5~~~~```~~ppa的源在ubuntu 18.04中已经不管用了,推荐直接用`sslocal`启动,安装方式如下:```shellsudo apt install shadowsocks# 创建 sslocal 本地配置文件mkdir ~/.sslocalvim ~/.sslocal/sslocal.json# &#123;# &quot;server&quot;: &lt;your_server_ip&gt;,# &quot;server_port&quot;: &lt;your_port&gt;,# &quot;local_address&quot;: &quot;127.0.0.1&quot;,# &quot;local_port&quot;: 1080,# &quot;password&quot;: &lt;your_service_pwd&gt;,# &quot;timeout&quot;: 300,# &quot;method&quot;: &quot;RC4-MD5&quot;# &#125;sslocal -c ~/.sslocal/sslocal.json 设置开机启动,crontab -e 1@reboot /usr/bin/python /usr/bin/sslocal -c /home/zhongjiajie/.sslocal/sslocal.json ss服务端可以配置多个账户登录，配置方式如下 123456789101112131415cat /etc/shadowsocks.json&#123; "server":"your_server_ip", # 你的端口对应的密码 "port_password":&#123; "8381":"pass1", "8382":"pass2", "8383":"pass3", "8384":"pass4" &#125;, "timeout":60, "method":"rc4-md5", "fast_open":false, "workers":1&#125; Chrome 配置 SwitchyOmega里面介绍了 SwitchyOmega 的简单配置使用，包括设置 ss 代理、配置国家防火墙、配置自动切换代理等 chrome谷歌浏览器，这个没有什么比较解释，很多插件和google的app都在chrome上面，下载方式有两种 1234wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key addsudo sh -c 'echo "deb http://dl.google.com/linux/chrome/deb/ stable main" &gt;&gt; /etc/apt/sources.list.d/google-chrome.list'sudo apt-get updatesudo apt-get install google-chrome 安装完成后打开chrome发现及时运行了ss客户端还是没有办法翻墙，这个和windows下比较不同，ubuntu下需要手动设置代理才能翻墙，但是发现代理软件需要登陆chrome应用市场下载，后来发现start Google Chrome with proxy command可以在命令行启动chrome，于是在命令行下运行google-chrome --proxy-server=&quot;socks5://127.0.0.1:1080&quot;然后chrome就能科学上网了 修改root角色密码12sudo passwd root# enter root password and repeat it 配置hosts文件123456789$ cat /etc/hosts# 微软onedrive的DNS被污染，增加两行204.79.197.217 onedrive.live.com134.170.108.152 skyapi.onedrive.live.com# IntelliJ Pycharm 检测是注册url0.0.0.0 account.jetbrains.com192.168.0.253 slave.isoft192.168.0.252 hdp.isoft zshzsh被称为终极shell，比ubuntu默认的bash要强大不少。根据Installing ZSH安装zsh 1234567891011sudo apt-get install zsh# check if installedzsh --version# check default shell in systemecho $SHELL# 下载 oh-my-zsh 项目来配置 zsh 会自动读取环境变量并且自动帮 zsh 进行设置wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh# make zsh as your default shellchsh -s $(which zsh) thefuckthefuck 是自动纠正上一个命令的命令行工具，经常用命令行的同学怎么可以错过 1234567sudo apt updatesudo apt install python3-dev python3-pipsudo pip3 install thefuck# 修改 .zshrc 文件eval $(thefuck --alias fuck)source ~/.zshrc tmux12# 安装sudo apt-get install tmux 配置文件 123456$ cat ~/.tmux.conf# 默认&lt;prefix&gt;是Ctrl+b，如果你觉得不好按可以调整为Ctrl+aunbind ^bset -g prefix 'C-a'# Tmux动态载入配置而不是重启 设一个快捷键&lt;prefix&gt;r来重新载入配置bind r source-file ~/.tmux.conf \; display-message "Config reloaded" 基本使用方式 tmux-concept相关概念 [[tmux-concept.png|tmux-concept]] &lt;prefix&gt;指的是tmux的前缀键,所有tmux快捷键都需要先按前缀键.它的默认值是Ctrl+b 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 快捷键帮助列表&lt;prefix&gt;?# 新建一个sessiontmux new -s &lt;session_name&gt;# 或者新建一个无名session然后重命名tmux&lt;prefix&gt;$# unicode 显示问题 启动tmux增加参数tmux -u# detach 退出 Tmux Session，回到父级 Shell&lt;prefix&gt;d# 在bash查看当前tmux服务有哪些sessiontmux ls# 根据session名字回去该sessiontmux a -t &lt;session_name&gt;# 在tmux里面列出所有session及window的树结构 可以通过左右键展开树状结构 可以查看并切换session及windows&lt;prefix&gt;s# 关闭sessiontmux kill-session -t &lt;session_name&gt;# 新建windows&lt;prefix&gt;c# 切换同一session的第n个windows&lt;prefix&gt;&lt;n&gt;# 关闭当前窗口&lt;prefix&gt;&amp;# 切换window&lt;prefix&gt;&lt;编号&gt; # 切换到指定编号windows&lt;prefix&gt;p # 切换至上一窗口&lt;prefix&gt;n # 切换至下一窗口&lt;prefix&gt;w # 通过window列表切换window# 重命名当前windows&lt;prefix&gt;,# 修改当前window编号&lt;prefix&gt;.# 水平分割窗口 形成两个Pane&lt;prefix&gt;%# 垂直分割窗口 形成两个Pane&lt;prefix&gt;\"# 关闭当前Pane&lt;prefix&gt;x# 最大化当前Pane 重复一次恢复正常&lt;prefix&gt;z# 显示Pane编号 在编号消失前输入对应的数字可切换到相应的Pane&lt;prefix&gt;q# 移动光标切换Pane&lt;prefix&gt;&lt;方向键&gt;# 调整pane大小&lt;prefix&gt;&lt;ctrl+方向键&gt; # 以1个单元格为单位调整当前pane边缘&lt;prefix&gt;&lt;alt+方向键&gt; # 以5个单元格为单位调整当前pane边缘# 切换 Pane 布局&lt;prefix&gt;&lt;space&gt;# 复制粘贴# 在Tmux中通过`&lt;prefix&gt;[`进入拷贝模式，按下&lt;space&gt;开始拷贝。然后用Vim/Emacs快捷键选择文本，按下&lt;Enter&gt;拷贝所选内容。然后通过`&lt;prefix&gt;]`进行粘贴 jqjq是是linux下的json格式化神器,如果有安装python的情况下也可以使用json.tool完成.echo &#39;{&quot;one&quot;: 1, &quot;two&quot;: 2}&#39; | python -m json.tool 12345678910111213# installsudo apt-get install jq# qiuck startecho '&#123;"one": 1, "two": 2, "name": "zhongjiajie"&#125;' | jq# 获取keyecho '&#123;"one": 1, "two": 2, "name": "zhongjiajie"&#125;' | jq '.name'# 嵌套解析cat json_test.txt | jq '.location.city'# 内建函数 # 获取所有的keycat json_test.txt | jq 'keys' # 判断某个可以是否存在cat json_test.txt | jq 'has("location")' pv复制时候显示完成百分比、传输速度、剩余时间、已用时间，用法和cp类似: pv src_file &gt; dest_file，参照如何使用 pv 命令监控 linux 命令的执行进度 guake有了tmux之后guake很少用了 按F12可以快速生成一个terminal，然后失去焦点后会自动缩进去的终端，在临时处理事情的时候非常有用sudo apt-get guake，这里给了一些Guake can not init!问题的处理方式。 PythonPython虚拟环境的安装ubuntu 系统内置两个版本的 Python ，之前一直用py2，但是考虑到20年py2就要停止维护了，所以打算上py3， ubuntu 16.04 默认两个版本的 python 都有安装，开发的时候一般都会用虚拟环境以免 ubuntu 原生的工具被破坏 123456789101112131415161718192021# 安装 pip3sudo apt-get install python-pip3pip3 --version# 安装 virtualenvpip install virtualenvvirtualenv --version# 安装 virtualenvwrapperpip install virtualenvwrapper# 编辑你的 .bashrc 或者 .zshrc 在最后面添加VIRTUALENVWRAPPER_PYTHON=$(which python3) # 这句一定要添加，不然会默认用python2 来解析# 或者加 VIRTUALENVWRAPPER_PYTHON='/path/to/python3'export WORKON_HOME=$HOME/.virtualenvssource $(which virtualenvwrapper.sh)# virtualenvwrapper 使用mkvirtualenv venvdeactivatermvirtualenv venv pip 换国内源官方源很好，但是下载速度比较慢，所以把 pip 默认的源从官方源改成国内源提高速度，等国内源下载确实有问题时再去官方源下载，国内源用的是清华大学pypi源 javaJava JDK有两个版本，一个开源版本Openjdk，还有一个oracle官方版本jdk。下面记录在Ubuntu 16.04上安装Java JDK的步骤 123456789101112# 开源版openjdksudo apt-get updatesudo apt-get install openjdk-8-jdkjava -version# oracle Java JDK# 安装依赖包sudo apt-get install python-software-properties# 添加仓库源sudo add-apt-repository ppa:webupd8team/javasudo apt-get updatesudo apt-get install oracle-java8-installer gradle根据gradle-instal方式,推荐手动安装,然后将安装路径的bin子路径放在环境变量中vim ~/.zshrc; export PATH=$PATH:/opt/gradle/gradle-4.10.2/bin,测试是否存在gradle -v maven安装mavensudo apt-get install mvn,测试是否成功及版本mvn -v vscode文本编辑器，根据官网Running VS Code on Linux的介绍，建议直接下载deb包并且安装 PycharmJetBrains 出品的 Python IDE,可直接在 ubuntu software中下载 IdeaJetBrains 出品的 Java IDE,可直接在 ubuntu software中下载 数据库管理工具 datagrip: 跨三个平台的 jetbrains 独立出来的数据库管理工具,可以除了配置指定的数据源之外,还可以配置驱动链接 hive kylin 等数据源.但是 NoSQL 还没有很好的解决方案,可直接在ubuntu software中下载 dbeaver: 跨三个平台的数据库管理工具,天生支持 hive sparkhive neo4j Greenplum等数据库,但是不支持 mongo,同样是需要下载jar包通过jdbc进行链接 dockerdocker 是非常好用的容器技术，可以说是改变了软件的交付和部署方式，docker 的安装如下 1234567891011121314151617181920212223242526272829303132333435363738# 卸载旧版本的dockersudo apt-get remove docker docker-engine docker.io# docker 可选的内核模块sudo apt-get updatesudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual# 使用 apt 镜像源安装sudo apt-get updatesudo apt-get install apt-transport-https ca-certificates curl software-properties-common# 通过国内源安装curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# 向 source.list 添加 docker 软件源sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# 安装 Docker CEsudo apt-get updatesudo apt-get install docker-ce# 启动 docker cesudo systemctl enable dockersudo systemctl start docker# 建立 docker 用户组sudo groupadd dockersudo usermod -aG docker $USER# 重启之后可以在普通用户运行 docker# 国内镜像加速vim /etc/docker/daemon.json # 输入你用的国内镜像加速器 这里以docker中国为例# &#123;# "registry-mirrors": [# "https://registry.docker-cn.com",# "https://&lt;your-specific-key&gt;.mirror.aliyuncs.com"# ]# &#125;sudo service docker restart 其中国内镜像加速器有docker中国阿里云daocloud 输入法update: 20180829,使用rime输入法替代搜狗输入法,用的是 fcitx-rime 1234sudo apt-get install fcitx-rimeim-configsudo rebootfcitx-config-gtk3# 重启后便可 搜狗输入法是我下载的第一个应用，他是和计算机交流的重要途径。直接前往搜狗for linux进行下载，安装指南直接看官网的安装指南。因为我是ubuntu 16.04.LTS，所以我更新了upgrade了系统的软件后直接双击.deb文件就完成了安装过程。 微信可直接在 ubuntu 18.04的 ubuntu software下载 生活沟通必备，主要是QQ被我用烂了，不想用了。两种方式可以实现微信的安装，一种是在chrome插件里面安装，直接页面操作不多解释。另一种是第三方的安装方式electronic-wechat，如果是简单的交流肯定是用第一种方式更加简单粗暴，如果希望有更多的功能就要选择第二中方式。 shutter截图软件，直接在ubuntu software搜索安装 安装之后不能在ubuntu任务栏上显示其快捷方式,Re-enable Shutter App Icon in Ubuntu 18.04 System Tray 安装完了之后不能对截图进行编辑,可以按照How to enable the Edit button in Shutter的方式安装三个依赖的包然后重启电脑便可 网易云音乐听歌必备神器，直接在网易云音乐下载安装点击deb包就可以安装 VLC视频播放软件，直接在ubuntu software搜索安装 Tweaks直接在ubuntu software安装 配置开关在左边: windows-&gt;titlebar buttons-&gt;placement-&gt;left 显示电池百分比: top bar-&gt;battery percentage 不显示应用菜单: top bar-&gt;application menu 调整显示时间相关: top bar 桌面不显示icon: desktop-&gt;show icon WPS先在WPS-社区下载对应的安装包,到对应的下载目录安装sudo dpkg -i wps-office_10.1.0.5672_a21_amd64.deb,如果此时安装出错要用sudo apt-get install -f.安装成功打开回提示部分字体缺失,可以到https://pan.baidu.com/s/1eS6xIzo下载字体,解压字体并sudo mv wps_symbol_fonts /usr/share/fonts移动到对应的文件夹.运行命令生成字体索引信息sudo mkfontscale;sudo mkfontdir更新字体缓存sudo fc-cache FBReader轻量级的阅读器,支持多种电子书籍格式,直接在应用中心下载 钉钉平时工作交流用钉钉，所以装了个钉钉 for linux，只需要按照README文档中介绍在relase页面下载相应的deb包进行安装就行 有道翻译翻译软件，直接有道翻译页面下载deb然后安装，建议下载deepin版本 数据库客户端Oracle client - sqlplus安装 Oracle client 参见 Installing Oracle SQL*Plus client on Ubuntu 以及 How to install SqlPlus 设置开机启动，进入search your computer，就是按Windows键或者ubuntu建，输入startup Remmina远程桌面客户端ubuntu自带软件,支持多个远程协议 RDP SFTP SSH VNC 迅雷通过迅雷固件制作而成的前端，需要自己编译，但是教程已经比较完整，详情查看Ubuntu上编译安装说明，安装成功启动后请看使用说明，因为有部分注意事项要遵守 由于迅雷前端固件经常会报错,所以直接用 xware + 迅雷远程下载网页版进行下载 1234docker pull zhongjiajie/docker-xwaredocker-compose up -ddocker logs -f# 可以在日志中看到对应的 `THE ACTIVE CODE IS`,将 百度网盘客户端百度网盘和迅雷在国内都比较常用，百度网盘 Linux 客户端在 github 上主要是三个bypyBaiduPCSYufeikang/bcloud，其中前两个是命令行客户端，后面是图形化界面。本来考虑图形化界面，但是看到 README 说作者不维护了，Yufeikang/bcloud 就是因为原作者没有维护所以才拿过来修改的，现在 Yufeikang 也弃坑了，所以只能转战命令行客户端了。两个来选，当然选择我更加熟悉的 Python 版的 bypy 了。说不定还能提几个 issue 和 PR 呢。 12345# 我放在了虚拟环境中运行workon venvpip3 install bypybypy list # 在命令行界面会给出提示，让你去指定的网址进行登录注册，并将授权码复制到命令行 桌面美化相关部分的桌面美化对于ubuntu来说还是有必要的，毕竟原来的桌面真的有点丑，我选择了大众而简单的美化路线，ubuntu tweakflatabulous-theme以及ultra-flat-icons。 123456789101112# ubuntu tweaksudo apt-get install unity-tweak-tool# flatabulous-theme 主题sudo add-apt-repository ppa:noobslab/themessudo apt-get updatesudo apt-get install flatabulous-theme# flatabulous配套图标sudo add-apt-repository ppa:noobslab/iconssudo apt-get updatesudo apt-get install ultra-flat-icons 安装完成后就可以启动ubuntu tweak，然后在Tweak -&gt; Theme中的GTK和windows主题中选择flatabulous，icon主题中选择ultra-flat pandoc格式转换工具pandoc,直接通过apt安装sudo apt-get install pandoc,将markdown转成HTML的例子如下pandoc -o test.html test.md rdesktoprdesktop is an open source client for Windows Remote Desktop Services, capable of natively speaking Remote Desktop Protocol (RDP) in order to present the user’s Windows desktop. 12sudo apt-get install rdesktoprdesktop [hostname] Ref shadowsocks-qt5安装指南 Want to start Google Chrome with proxy command How to install guake (dropdown terminal) on Xubuntu 14.04 XwareDesktopUbuntu上编译安装说明 Chrome 配置 SwitchyOmega docker - use-case-the-china-registry-mirror Linux下Json格式化神器jq Installing Oracle SQL*Plus client on Ubuntu How to install SqlPlus 如何使用 pv 命令监控 linux 命令的执行进度 迅雷远程下载网页版 datagrip dbeaver rime-download gnome-extensions How to enable the Edit button in Shutter Re-enable Shutter App Icon in Ubuntu 18.04 System Tray gradle-instal]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用命令]]></title>
    <url>%2F2017%2F10%2F24%2Fgit-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[背景NOTE!: github page根据使用功能区分，想要了解更多见本人wiki 本文介绍Git常见的命令以及部分问题的处理方式，是一篇应用性较强的文章，如果想要更加深入的了解Git的原理，请参考本文的References 工作流 配置基本配置在使用Git前要对其进行基本的配置，记录是谁进行当前的修改 12git config user.name "YOUR_NAME" # 设置 commit 的用户git config user.email "YOUR_EMIAL@example.com" # 设置 commit 的邮箱 个性化配置12345# 个性化 loggit config --global alias.lg "log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative" # 自定义lg命令 格式化log的输出# 个性化 refloggit config --global alias.rlg "reflog --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative" 获取版本库从远程版本库获取从远程版本库中直接拉取一个仓库，会在当前路径下新生成一个文件夹 12git clone https://github.com/zhongjiajie/Autohome.git # 从github获取git clone git@server-name:path/repo-name.git # 从私有服务器中获取 新建版本库一般做法，只能在本地提交、推送到远程，别的用户不能从该库进行clone以及push操作，看到first_git文件夹下面有.git隐藏文件夹就说明创建成功 123mkdir first_gitcd first_gitgit init 新建裸库新建远程版本库，可以提供clone和push到改版本库功能。此时first_git文件夹中和新建版本库不同，会生成多个文件夹 123mkdir first_gitcd first_gitgit init --bare 远程版本库查看远程版本库12git remote # 查看远程版本库信息git remote -v # 远程版本库详细信息 添加远程版本库1git remote add origin git@server-name:path/repo-name.git # 添加一个远程库 推送到远程版本库12git push origin master # 推送到远程master分支git push -u origin BRANCHNAME # 本地新建分支推送到远程兵追踪 关联远程分支1git checkout -b dev origin/dev # 新建本地dev分支 与远程origin/dev分支进行关联 切换到本地dev分支 抓取分支的修改123git fetch origin master # 抓取远程master分支的commit信息git pull origin master # 抓取远程master分支的commit信息并对文件进行相应修改git fetch origin pull/ID/head:BRANCHNAME # 抓取远程分支的某个pull requests 删除远程分支12git push origin --delete BRANCHNAME # --delete选项删除远程分支git push origin :BRANCHNAME # 推送一个空的分支到要删除的远程分支 更新远程分析信息1git fetch -p # 更新远程版本库同步分支和tag 分支管理新建分支1234git branch dev master # 在master的基础上创建dev分支git checkout dev # 切换到dev分支git checkout -b dev master # 在master的基础上创建dev分支 并切换到dev分支git checkout -b dev origin/dev # 新建dev分支 与远程origin/dev分支进行关联 并切换到dev分支 合并分支12345git checkout mastergit merge --no-ff dev # 合并dev分支 no-ff选项保留原分支记录 commit按照时间排序git checkout devgit rebase master # 合并dev分支到master分支的最后面 重新排列commit顺序 删除分支12git branch -d dev # 删除已合并的分支git branch -D dev # 强行删除分支 修改和提交状态和差异12345git status # 查看工作区、暂存区的状态git diff # 查看未暂存的文件更新git diff --cached # 查看已暂存文件的更新git diff HEAD -- FILENAME # 查看工作区和版本库里面最新版本的区别git diff BRANCH_1 BRANCH_2 # 在合并改动之前，预览两个分支的差异 删除和移动123456git rm FILENAME # 直接删除文件git rm --cached FILENAME # 删除文件暂存状态git mv FILENAME_1 FILENAME_2 # FILENAME_1重命名成FILENAME_2git mv FILENAME_1 path/to/FILENAME_2 # FILENAME_1移动到path/to目录并重命名成FILENAME_2git rm FILENAME # 删除文件FILENAME 储藏和恢复123456git stash # 储藏当前工作状态git stash list # 查看储藏列表git stash apply STASH_ID # 恢复指定储藏状态git stash drop STASH_ID # 删除指定储藏IDgit stash pop # 恢复最近储藏 并删除 相当于apply + dropgit stash clear # 清空储藏列表 提交修改12345git add FILENAME # 添加文件到暂存区git add . # 添加当前全部修改到暂存区git commit -m 'commit log' # 将暂存区修改提交到本地版本库并记录commit loggit commit -amend # 修改最后一次提交git push origin master # 提交到远程版本库 修改提交人信息1git commit --amend --author "YOUR_NAME &lt;YOUR_EMIAL@example.com&gt;" # 修改上次提交的用户信息 历史和回退历史1234git log --oneline # 查看历史 每个历史一条记录git log --oneline FILENAME # 查看指定文件历史 每个历史一条记录git log -p -2 # 显示最近2次提交内容的差异git show COMMIT_ID # 查看某次修改 回退12345678git checkout -- FILENAME # 丢弃工作区上某个文件的修改git reset HEAD FILENAME # 丢弃暂存区上某个文件的修改，重新放回工作区git reset --hard HEAD^ # 回退到上一个版本git reset --hard COMMIT_ID # 回退到具体某个版git reflog # 查看命令历史 常在merge或rebase丢失了commit后使用git revert COMMIT_ID # 撤销指定的提交 标签新建标签123git tag v0.1 # 新建标签，默认位 HEADgit tag v0.1 COMMIT_ID # 对指定的 commit id 打标签git tag -a v0.1 -m 'version 0.1 released' # 新建带注释标签 查看标签12git tag # 显示所有标签git show TAGNAME # 显示指定标签信息 删除标签12git tag -d TAGNAME # 删除标签git push origin :refs/tags/TAGNAME #删除远程标签 其他操作123git checkout TAGNAME # 切换到标签git push origin TAGNAME # 推送分支到源上git push origin --tags # 一次性推送全部尚未推送到远程的本地标签 FAQ环境配置问题windows下git bash中文乱码解决办法 修改 修改某次特定commit的作者Change commit author at one specific commit 修改某次特定的commit内容How to modify a specified commit in git 修改版本库的第一个commitEdit the root commit in Git 统计功能统计每个用户提交的次数 1git shortlog -nes # 统计每个用户提交的次数 name + email + summary 恢复 恢复所有在git中被删除的文件 1git ls-files -d | xargs echo -e | xargs git checkout -- 其他 How to link to specific line number on github ReferencesGit 常用技能my-gitGit教程Modifying an inactive pull request locally]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用github管理blog]]></title>
    <url>%2F2017%2F10%2F23%2F%E7%94%A8github%E7%AE%A1%E7%90%86blog%2F</url>
    <content type="text"><![CDATA[背景update at 2019-01-11: 由于这种方式过于分散,导致换了电脑后就难以恢复博客内容,所以放弃了.根据一个懒码农的经验,写博客需要的是耐心,而不是高明的博客内容管理方法.所以我现在使用的方法是: 在个人github-page-repo上面新建一个分支source用于储存全部的源码(包括主题和配置),需要写博客时直接切换到这个分支写完,然后hexo d完成部署 如果有多篇文章同时在编写中,没有完成release的,统一将内容放在本人的wiki的blog-tmp文件夹中,这样wiki在本地提交到repo中也会因为没有暴露出blog-tmp的文件夹链接而没有直接入口 自从开始写博客以来，怎么管理博客文章的编写和发布一直困扰着我，最近刚好在加强Gitflow工作流，正好将其运用到博客管理上，使得博客的编写和发布更加有条理，现将个人对博客的管理经验与大家分享 为什么要管理文章我写博客一般是下班或者午饭过后，都是一些零碎的时间。博客的内容有深有浅，有的能在20分钟内写完，有的要比较长的时间。如果遇到读书笔记之类的，要读完一本书才能完成一篇博客的就时间更长了 在写博客的时候很可能是多篇博客一起写，我目前有两篇读书笔记还在进行（长期），还有一个关于个人项目的博客（中长期），还有一些短期的想法想要分享，例如这篇博客 这样在博客发布的时候就笔记复杂了，之前的做法是认为将没有完成的文章剪切到别的地方，然后将已完成的博客用HEXO进行部署，这样的方法显得不够灵活，操作复杂且容易丢失博客，试过吃完饭睡个觉忘了把剪切出来的博客复制回去，之前时间太久博客被自己手贱删了。所以就有了用Gitflow来管理博客的想法，通过建立不同的分支来控制各个每篇不同的文章，完成之后再合并到master分支用于发布，更好管理文章的同时也可以加强写git的使用，何乐而不为？ Gitflow博客管理流程创建repository在Github上创建存储blog的repository，我创建了一个srcBlog 对文章的管理 先将之前已经完成并发布的文章放在master分支中 从master分支中checkout一个develop分支 新建文章就从develop分支中checkout一个分支进行编写，分支名为feature-&lt;post_name&gt; 完成了文章的创建就checkout到develop分支，git merge --no-ff feature-&lt;post_name&gt; 对于没有完成的文章，或者持续更新的文章（读书笔记、学习日志等），及时的git stash储存或者git push -u feature-&lt;post_name&gt;推送到Github 对特定文章的编写再git checkout feature-&lt;post_name&gt; Gitflow工作流 ReferenceGit分支管理策略]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>Github</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pypi安装包制作及发布]]></title>
    <url>%2F2017%2F10%2F21%2Fpypi%E5%AE%89%E8%A3%85%E5%8C%85%E5%88%B6%E4%BD%9C%E5%8F%8A%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[前言介绍如果将自己的程序制作成pypi包并发布到pypi pypi简介pypi全程是Python Package Index，其官方定义是 The Python Package Index is a repository of software for the Python programming language. There are currently 119827 packages here. – until 2017-10-21 我们平常安装程序所用的pip就是到pypi中找到相应的库并将其安装到本地的，比如我们运行 1pip install requests pip就会去pypi找到最符合requests的这个包，然后下载到本地，最后将其安装 pypi安装包制作注意pypi的库不允许有相同的名称，如果想要跟着教程完成相关的操作的话请先将教程的zhongjiajie改成你的名称（目的是在pypi找不到相应的库），才能完成库的上传操作 普通库这里的普通库指的是通过from package import function或import package方式调用的库，下面进行具体程序的演示，先切换到项目目录，这里是myproject 创建项目1234cdcd Desktop/mkdir myprojectcd myproject/ 定义库的功能创建文件夹zhongjiajiepypi并且在其中编写简单的Python函数myfunction.py及定义init.py 1234mkdir zhongjiajiepypicd zhongjiajiepypi/vim __init__.py # 这个文件作用就是给这个文件夹打成包vim myfunction.py # 这里放置逻辑代码了 其中myfunction.py的内容为 12345678910cat myfunction.py#!/usr/bin/env python#-*- coding:utf-8 -*-def mysum(*args): s = 0 for v in args: i = float(v) s += i print s init.py的内容为 12$ cat __init__.pyfrom myfunction import mysum 编写项目setup.py文件这是重点：接着要编写pypi的setup文件，生成pip可以识别的格式，为后期上传到pypi做准备，我们先切换到myproject根目录，然后编辑setup.py文件 123456789101112131415161718192021222324$ vim setup.py # 写入相应的setup.py内容 详见cat命令结果$ cat setup.py#!/usr/bin/env python#-*- coding:utf-8 -*-from setuptools import setup, find_packagessetup( name = "zhongjiajiepypi", version = "0.0.1", keywords = ("pip", "testpypi"), description = "test pip module", long_description = "test how to define pip module and upload to pypi", license = "MIT", url = "https://zhongjiajie.github.io", # your module home page, such as author = "zhongjiajie", # your name author_email = "zhongjiajie955@hotmail.com", # your email packages = find_packages(), include_package_data = True, platforms = "any", install_requires = []) 这里我们需要注意一下，setup.py文件是能不能正确生成pip能识别文件的关键，他有自己的一套格式，我们编写setup文件要遵循这一套格式，实例代码是基础的设置，更多设置请查阅Writing the Setup Script 打包pypi此时整个包已经编写完成了，整个项目的文件结构是 123456$ tree.├── zhongjiajiepypi│ ├── __init__.py│ └── myfunction.py└── setup.py 现在我们需要将项目变成pip能够识别并安装的文件，在本地测试成功后将文件上传到pypi，在项目的根目录（setup.py同等位置）中运行 12python setup.py sdist # 生成二进制包 支持pip安装 推荐使用python setup.py bdist_egg # 生成egg 支持easy_install安装 Python会自动将项目打包成二进制包，此时项目的文件结构是 12345678910111213tree.├── dist│ └── zhongjiajiepypi-0.0.1.tar.gz├── zhongjiajiepypi│ ├── __init__.py│ └── myfunction.py├── zhongjiajiepypi.egg-info│ ├── dependency_links.txt│ ├── PKG-INFO│ ├── SOURCES.txt│ └── top_level.txt└── setup.py 此时已算完成了pypi的打包 本地测试建议在将包上传到pypi之前现在本地完成测试工作，方法是先进入dist文件夹，然后用pip命令安装本地的二进制包，安装完成后测试其中的方法是否可用 1234$ cd dist$ pip install zhongjiajiepypi-0.0.1.tar.gz$ python -c "import zhongjiajiepypi; zhongjiajiepypi.mysum(3, 4, 5)"12.0 可以看到结果是12.0证明包能达到预期，可以上传到pypi 上传到pypi注意: 部分旧版教程会说先运行python setup.py register进行注册，然后再运行python setup.py sdist upload将二进制包上传到pypi上，但是目前pypi已经可以不用注册直接运行python setup.py sdist upload完成上传。 我们先尝试用python setup.py register，接着输入密码，结果收到下面的提示 12Registering zhongjiajiepypi to https://upload.pypi.org/legacy/Server response (410): Project pre-registration is no longer required or supported, so continue directly to uploading files. 可以看到确实不用先注册了，下面我们尝试直接运行python setup.py sdist upload，然后输入密码，结果让我吃了一斤 123Submitting dist/zhongjiajiepypi-0.0.1.tar.gz to https://upload.pypi.org/legacy/Upload failed (403): Invalid or non-existent authentication information.error: Upload failed (403): Invalid or non-existent authentication information. 两种方法都不能将本地包上传到pypi，于是我google了原因，看到了github上面相关的链接Invalid or non-existent authentication information，要新建$HOME/.pypirc文件用于保存pypi的连接信息，尝试了@dover247的文件样例发现仍不能上传到pypi，然后看到@jaraco说那个版本有点旧，就去了The Python Package Index (PyPI)使用了最新的样例，然后再运行python setup.py sdist upload就能上传成功了（注意国内网络环境访问pypi） CLICLI简介CLI（command-line interface，命令行界面）是指可在用户提示符下键入可执行指令的界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。我个人的理解是通过命令行操作调用程序的接口，以达到我们的目的，好处是更加直观、并提供交互式的完成一些简单的操作 定义Python的CLI接口Python的CLI和Python的一般库的做法类似，不同的是它暴露了一个CLI的关键字，调用起来就类似于调用python script_name.py args的形式。他会在$PYTHON_HOME/Scripts生成可运行文件，命令行调用，下面我们把之前的例子改成CLI的调用方式，先修改zhongjiajiefunction.py文件，加上命令行传参的功能。 12345678910111213141516$ cat zhongjiajiefunction.py#!/usr/bin/env python#-*- coding:utf-8 -*-def mysum(*args): s = 0 for v in args: i = float(v) s += i print s# 从此是新加的功能def mysum_cli(): import sys args = sys.argv[1:] mysum(*args) 新加的mysum_cli提供了在命令行获取相关的参数，并将参数传递给原来的mysum函数。接着修改setup.py函数。 123456789101112131415161718192021222324252627282930$ cat setup.py#!/usr/bin/env python#-*- coding:utf-8 -*-from setuptools import setup, find_packagessetup( name = "zhongjiajiepypi", version = "0.0.1", keywords = ("pip", "testpypi"), description = "test pip module", long_description = "test how to define pip module and upload to pypi", license = "MIT", url = "https://zhongjiajie.github.io", author = "zhongjiajie", author_email = "zhongjiajie955@hotmail.com", packages = find_packages(), include_package_data = True, platforms = "any", install_requires = [], # 此处起是增加的内容 entry_points = &#123; 'console_scripts': [ 'mysum=zhongjiajiepypi.zhongjiajiefunction:mysum_cli', ] &#125;) 可以看到只需要在配置中加上entry_points入口选项就行了，其中console_scripts中指定命令行中的关键字，此处为mysum，对应的是zhongjiajiepypi这个库的zhongjiajiefunction函数里面的mysum_cli方法。 至此完成了CLI的配置，接着根据之前介绍的打包pypi的方法进行打包，然后本地测试运行就能得到我们想要的结果了，当和下面一样看到命令行调用mysum 2 3 4的结果是9.0的时候就说明CLI接口已经能正常运行了 12345$ python setup.py sdist$ cd dist$ pip install zhongjiajiepypi-0.0.1.tar.gz$ mysum 2 3 49.0 Tips 由于pypi不能有同名的库，所以拿样例测试的时候要将名字改成pypi不存在的库名 上传到pypi已经不用先注册，但要新建$HOME/.pypirc文件填入适当的内容 References怎么制作pip安装包，Python Egg 如何将自己的程序发布到 PyPI How To Package Your Python Code]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pypi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美女图片API]]></title>
    <url>%2F2017%2F08%2F30%2F%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87API%2F</url>
    <content type="text"><![CDATA[背景近期重构了之前Github的一个项目beauty，项目伊始是大学期间为了学习Python及爬虫。项目通过爬虫提供了API访问55156图库-高清套图，让整个浏览图片的过程显得更加流畅与愉悦 项目更新 20171017: 上传代码到pypi，提供更加便捷的使用方式 项目地址zhongjiajie/beauty 设计理念该项目爬虫的难度不大，但是提供了一种更加流畅与愉悦的看图方式，可以供大家参考。重构原因：大学期间的爬虫是用了多线程直接把55156图库-高清套图中的所有图片下载下来，已经达到了爬虫的学习目的但是只能全量抓取，不能有节制的爬取，一是容易带来资源的浪费（跑很久），二是部分的套图质量确实不敢恭维。所以重构的时候就秉持着几个原则： 提供选择下载什么分类的套图（ugirl、tuigirl等） 提供选择现在多少个最新的套图 提供先下载单张小图，然后肉眼筛选再指定下载整个套图 如何使用clone或下载源码从github版本库zhongjiajie/beauty中clone或下载相应的源码 安装依赖项目根目录运行 1pip install beauty 快速入门tips: 以下命令均在项目根目录运行 查看Usage及支持的套图类型1beauty -h Usage： 得知项目主要有两个方法scan和download，其中scan是浏览指定种类和数量的封面图片（小图），download是下载指定种类和数量的套图（大图）。 Arguments： 目前支持的套图种类，为scan和download方法的必填项 Options： 参数的关键字及对应的解释 直接下载套图12345678# 下载秀人套图 默认数量为10 默认路径是./picbeauty download xiuren# 下载秀人套图 指定数量为15beauty download xiuren -n 15# 下载秀人套图 指定路径为D:/beauty download xiuren -p D:/ 完成后可在相应目录下看到下载结果 先浏览小图，根据个人喜好（套图质量）下载对应的大图12345678# 浏览秀人套图 默认数量为10 默认路径是./pic/scanbeauty scan xiuren# 浏览秀人套图 指定数量为15beauty scan xiuren -n 15# 浏览秀人套图 指定路径为D:/scan scan方法默认的图片会下载到指定（默认目录）的scan文件夹beauty download xiuren -p D:/ 找到浏览套图文件夹scan，打开图片编辑器查看scan文件夹的图片 从上图中找到要完整下载的套图，运行下载命令 12# -n参数要和scan方法的参数一样 -f参数列表内部不要留空格beauty download xiuren -n 10 -f [204774,204775,204832] 完成后可在相应目录下看到下载结果 代码分析命令行参数库docopt获取命令行参数用了docopt第三方库，docopt通过解析py文件的__doc__文档生成对应的命令行参数解析。 123456789101112131415import docopt"""Usage: beauty.py scan &lt;NAME&gt; [-n=&lt;num&gt;] [-p=&lt;path&gt;] beauty.py download &lt;NAME&gt; [-n=&lt;num&gt;] [-f=&lt;list&gt;] [-p=&lt;path&gt;] beauty.py (--help | -h) beauty.py --versionOptions: --help -h Show this screen. --version Show version. --num -n=&lt;num&gt; Number of picture to download, [default 10]. --filter -f=&lt;list&gt; Filter from parameter --num album number --path -p=&lt;path&gt; Path to download picture, [default `$project/pic/`].""" 上面的代码会自动对命令行的输入进行解析，解析的结果会保存在一个dict类型里面，如我运行 1beauty download tuigirl -n 10 -p ./path docopt会自动根据__doc__已经其中的Usage解析，将获取的参数存储成Python中的一个dict 12345678910&#123; '--filter': None, '--help': False, '--num': '10', '--path': './path', '--version': False, '&lt;NAME&gt;': 'tuigirl', 'download': True, 'scan': False&#125; 爬虫主程序分析项目主要依赖于Python2.7开发环境，如果发现自己环境运行不起来，欢迎在github上提issues 爬虫程序用到的第三方库是requests，提取html中的数据用的是re内置库 初始化输入及re.compile创建实例的时候先对输入进行判断，根据输入执行相应的操作。同时生成多个re.compile对象，用于后期对html的匹配 12345678910111213141516171819202122232425# 生成re.compile对象，用于后期html的提取_SMALL_PIC_TMP_REGEX = r'&lt;div class="listBox" id="imgList"&gt;.*?&lt;/div&gt;'_SMALL_PIC_NAME_REGEX = r'&lt;a href="(.*?)" title="(.*?)" class="picLink"&gt;&lt;img src="(.*?)"'_SAMLL_NEXT_PAGE_REGEX = r'&lt;a target=\'_self\' href=\'(.*?)\'&gt;下一页&lt;/a&gt;&lt;/li&gt;'_BIG_PIC_REGEX = r'&lt;img alt=".*?" src="(.*?)" /&gt;'_BIG_NEXT_PAGE_TMP_REGEX = r'&lt;div class="pages"(.*?)&lt;/ul&gt;'_BIG_NEXT_PAGE_REGEX = r'&lt;a href=\'(.*?)\''_SMALL_PIC_TMP_PATTERN = re.compile(_SMALL_PIC_TMP_REGEX, re.S)_SMALL_PIC_NAME_PATTERN = re.compile(_SMALL_PIC_NAME_REGEX)_SAMLL_NEXT_PAGE_PATTERN = re.compile(_SAMLL_NEXT_PAGE_REGEX)_BIG_PIC_PATTERN = re.compile(_BIG_PIC_REGEX)_BIG_NEXT_PAGE_TMP_PATTERN = re.compile(_BIG_NEXT_PAGE_TMP_REGEX, re.S)_BIG_NEXT_PAGE_PATTERN = re.compile(_BIG_NEXT_PAGE_REGEX)def __init__(self, url, scan=False, download=False, filter=None, path=None, num=None): """初始化参数及正则表达式""" self.url = url self.is_download = download self.is_scan = scan self.filter_list = eval(filter) if filter else None # 默认下载路径是项目根目录./pic self.path = path if path else '&#123;dir&#125;/pic'. \ format(dir=os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) self.num = int(num) if num else 10 # 0下载全量 其他下载指定数量 获取网页response先通过random.choice方法随机获取文件./beauty/user_agent.py文件中的UA，再通过requests.get方法获取获取网页的response，由于是整个项目获取response的方法，所以要函数的输入的URL应该定义为可变参数，并将函数设置成@staticmethod方法 123456789@staticmethoddef get_response(url): """获取网页源代码""" headers = &#123; 'User-Agent': random.choice(agents) &#125; response = requests.get(url, headers=headers) return response 解析response主方法该函数主要根据用户输入解析成不同的执命令，scan和download。scan指下载封面的小图片，download下载整个套图的图片，并检查获取的数量是否大于指定的获取数量 12345678910111213141516171819202122232425262728293031323334353637def parse(self): """Beauty主函数""" # 获取封面图片URL，套图名称，大图URL big_pic_name = [] html = self.get_response(self.url).content big_pic_name.extend(self._parse_small_pic(html)) # 判断获取的套图数量是否大于指定数量 while len(big_pic_name) &lt; self.num: samll_next_page = self._parse_small_next_page(html) html = self.get_response(samll_next_page).content big_pic_name.extend(self._parse_small_pic(html)) # 生成以套图编号为key的dict pic_no_dct = &#123;os.path.basename(i[0]).split('.')[0]: list(i) for i in big_pic_name[:self.num]&#125; # 下载指定album if self.filter_list: pic_no_dct = &#123;key: pic_no_dct[key] for key in pic_no_dct if int(key) in self.filter_list&#125; # 下载图片 if self.is_download: for album in pic_no_dct: big_pic_url = pic_no_dct[album][0] album_name = pic_no_dct[album][1].decode('utf8') html = self.get_response(big_pic_url).content for url in self._parse_big_pic(html, url=big_pic_url): pic_no_dct[album].append(url) folder = u'&#123;album&#125;_&#123;name&#125;'. \ format(album=album, name=album_name) self._download_pic(folder, pic_no_dct[album][3:]) # 浏览图片 elif self.is_scan: scan_dct = &#123;key: pic_no_dct[key][2] for key in pic_no_dct&#125; self._scan_pic(scan_dct) 解析大小图片解析大小图片，根据parse方法的判断分别执行相应的解析函数 1234567891011121314151617181920212223242526272829303132333435def _parse_small_pic(self, html): """解析小图""" mid_html = self._SMALL_PIC_TMP_PATTERN.findall(html)[0] pic_url_name = self._SMALL_PIC_NAME_PATTERN.findall(mid_html) return pic_url_namedef _parse_small_next_page(self, html): """解析下一页小图""" part_url = self._SAMLL_NEXT_PAGE_PATTERN.findall(html)[0] return self._subsite_url(part_url)def _parse_big_pic(self, html, **kwargs): """解析大图""" for url in self._parse_big_sub(html, **kwargs): yield urldef _parse_big_sub(self, html, **kwargs): """解析大图子函数""" next_page_mid = self._BIG_NEXT_PAGE_TMP_PATTERN.findall(html)[0] next_page = self._BIG_NEXT_PAGE_PATTERN.findall(next_page_mid)[-1] yield self._BIG_PIC_PATTERN.findall(html)[0] while next_page != '#': html = self.get_response(os.path.split(kwargs['url'])[0] + '/' + next_page).content next_page_mid = self._BIG_NEXT_PAGE_TMP_PATTERN.findall(html)[0] next_page = self._BIG_NEXT_PAGE_PATTERN.findall(next_page_mid)[-1] yield self._BIG_PIC_PATTERN.findall(html)[0]def _subsite_url(self, part_url): """返回子页面完整url""" return self.url + part_url Star&amp;Fork如果你觉得有点意思或者使用起来比较方便，欢迎在github上star我的项目zhongjiajie/beauty如果你还有别的需求，或者觉得有需要改进的地方，欢迎提issues。如果你想要学习代码获取对项目进行协同开发欢迎Fork或者pull requests]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法相关]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%AE%97%E6%B3%95%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[前言分享一些平时遇到算法相关的程序，其中包括基础的数据算法，矩阵相关算法等 数据算法相关随机返回列表的全部元素写一个函数，随机返回输入列表的全部元素 1234567import randomdef random_lst_val(lst): len_ = len(lst) while len_ &gt; 0: yield lst.pop(random.randint(0, len_ - 1)) len_ = len_ - 1 最长的没有重复字母的子字符串给一个字符串，找出长度最长的且没有重复字符的子字符串，如：abcabcbb-&gt;abc长度是3,bbbbb-&gt;b长度是1,pwwkew-&gt;wke长度是3 123456789101112def longest_substring(str_): tmp_longest_word = &#123;&#125; word = '' for i in str_: if i not in word: word += i else: tmp_longest_word[len(word)] = word word = i longest_idx = max(tmp_longest_word.keys()) return tmp_longest_word[longest_idx], longest_idx 将矩阵0元素所在行列置0给定一个0、1矩阵，请写一个矩阵转换函数，将所有0所处的行和列所有元素置为0，并分析时间复杂度： 1231 0 0 0 0 01 0 1 --&gt; 0 0 01 1 1 1 0 0 Python函数 通过Numpy计算 12345678910import numpy as npdef tran_a_row_col_zero(a): """ :param a: array """ i, j = np.where(a==0) a[i, :] = 0 a[:, j] = 0 return a 用内置库计算 123456789101112131415161718192021222324import copylst = [[1, 0, 0], [1, 0, 1], [1, 1, 1]]def change_row_col2zero(arr, i, j): len_ = len(arr) arr[i] = [0] * (len_) for idx in range(len_): arr[idx][j] = 0 return arrdef transform_list(l): len_ = len(l) len_sub = len(l[0]) copy_ = copy.deepcopy(lst) for i in range(len_): for j in range(len_sub): if lst[i][j] == 0: change_row_col2zero(copy_, i, j) return copy_print transform_list(lst) 时间复杂度 用Numpy时间复杂度未知 用内置库，如果矩阵是方阵且行数为n: O(n^3) 阿姆斯特朗数一个n位正整数等于其各位数字的n次方之和,则称该数为阿姆斯特朗数，例如1^3 + 5^3 + 3^3 = 153，写一个函数，计算所有给定范围内的阿姆斯特朗数 123456789101112131415161718192021222324252627282930def armstrong_number(*args): """用户输入范围的armstrong_number :param args: :type args: tuple :return: """ # 检查用户输入数量 if len(args) == 3: start, end, step = args if len(args) == 2: start, end = args step = 1 if len(args) == 1: end = args[0] start, step = 0, 1 for num in xrange(start, end, step): # 指数位数和 sum = 0 # 指数 n = len(str(num)) temp = num while temp &gt; 0: digit = temp % 10 sum += digit ** n temp //= 10 if num == sum: yield num 斐波那契数列斐波那契数列定义：F(0)=0，F(1)=1, F(n)=F(n-1)+F(n-2)，例如0,1,1,2,3,5…给一个通用函数，输入数列最大值，得到一系列斐波那契数列 12345def fab(max): a, b = 0, 1 while a &lt;= max: yield a a, b = b, a + b 任意连续整数阶乘的和12def add_factorial(n): return reduce(lambda x, y: x + y, map(lambda x: x * x, range(1, n + 1))) 任意连续区间的素数、质数1234def is_prime(start, stop): # 要求x大于2并且x在区间[2, x)中没有整除的除数 return filter(lambda x: x &gt;= 2 and not [x for i in range(2, x) if x % i == 0], range(start, stop + 1)) # 取出质数,x从range(start,stop) 取的数 字典中最大的value对应的keys12345def dct_max_val_key(dct): max_value = max(dct.values()) keys = [k for k, v in dct.items() if k == max_value] # 最大值对应的key可能有一个或者多个 return keys[0] if len(keys) == 1 else keys 字符串最长且相邻元素不相同的子字符串找出一个字符串中最长的相邻字符串中不重复的子字符串，并输出字符串的长度。如 11101111，最长是3，子字符串是101。110011 最长是2 子字符串是10 或 01 12345678910111213141516171819202122232425262728293031323334def find_large_sub_str(string): """只能找到最长的长度，当有多个值重复的时候不能找到全部最长的子字符串""" sub_str = '' pre = None dct = &#123;&#125; for i in string: if pre != i: sub_str += i else: dct[len(sub_str)] = sub_str sub_str = pre pre = i max_key = max(dct.iterkeys()) return max_key, dct[max_key]def find_large_sub_str_1(string): """能找到重复的长度的子字符串""" sub_str = '' pre = None dct = &#123;&#125; for i in string: if pre != i: sub_str += i else: dct[sub_str] = len(sub_str) sub_str = pre pre = i max_val = max(dct.itervalues()) keys = [k for k, v in dct.items() if v == max_val] return max_val, keys[0] if len(keys) == 1 else keys 字符串str中的连续最长的数字串读入一个字符串str，输出字符串str中的连续最长的数字串。例如：abcd12345ed125ss123456789 输出：123456789 123456789101112131415161718192021def str_longest_int(s): result_dict = &#123;&#125; int_set = set([str(i) for i in range(0, 10)]) tmp = '' for i in s: if i in int_set: tmp += i elif tmp: result_dict[tmp] = len(tmp) tmp = '' # 防止最后还有一个值 if tmp: result_dict[tmp] = len(tmp) max_val = max(result_dict.values()) key = [key for key in result_dict if result_dict[key] == max_val] return key[0] if len(key) == 1 else keys = 'abcd12345ed125ss123456789'print str_longest_int(s) 字符数组最后一个单词长度输入：”Hello World “。输出：5 1234567891011121314151617181920def last_word_len(s): len_ = len(s) curr = 0 index = -1 tmp = '' while curr &lt; len_: if s[index] != ' ': tmp += s[index] # 避免倒数第一个是' ' elif index != -1: return len(tmp) index -= 1 curr += 1 # 只要一个单词的情况 return len(s.strip())s = "Hello_World "print last_word_len(s) 数组中找指定和数组A由1000万个随机正整数(int)组成，设计算法，给定整数n，在A中找出a和 b，使其符合如下等式：n = a + b 123456789101112131415161718192021222324252627282930313233"""题目数据量大看似吓人 但是还是要透过现象看本质set用于去重 如果set太大也可以改成bitmap"""def two_sum_1(a, n): """标准leetcode的twosum问题""" mid_dict = &#123;&#125; for val in a: if val in mid_dict: return val, mid_dict[val] else: mid_dict[n - val] = val return Nonedef two_sum_2(a, n): """根据问题调整成set格式""" mid_set = set() for val in a: if val in mid_set: return val, n - val else: mid_set.add(n - val) return Nonea = [1, 2, 3, 4, 5, 6, 7, 8, 9]n = 10print two_sum_1(a, n)print two_sum_2(a, n) 判断重量给定两颗钻石的编号g1、g2，编号从1开始，同时给定关系数组vector，其中元素为一些二元组，第一个元素为一次比较中较重的钻石的编号，第二个元素为较轻的钻石的编号。最后给定之前的比较次数n。请返回这两颗钻石的关系，若g1更重返回1，g2更重返回-1，无法判断返回0。输入数据保证合法，不会有矛盾情况出现。输入：2,3,[[1,2],[2,4],[1,3],[4,3]],4 返回:1 123456789101112131415161718192021222324252627282930"""* 根据描述简历一个有向图 在图中找到指定两点之间是否有路径* 钻石编号是顶点，比较关系是边，生成有向图。先做两个顶点是否可达判断，再进行拓扑排序"""def bigger(x, y, l, n): """用递归调用 看是否有大小关系""" for i in range(n): if l[i][0] == x: if l[i][1] == y: return 1 else: return bigger(l[i][1], y, l, n)def compare(g1, g2, p, n): """要经过两次比较 比价g1 g2谁更大""" tag = bigger(g1, g2, p, n) # 先交换后比较 if tag != 1: tag = bigger(g2, g1, p, n) if tag == 1: tag = -1 else: tag = 0 return tagprint compare(2, 3, [[1, 2], [2, 4], [1, 3], [4, 3]], 4)print compare(3, 2, [[1, 2], [2, 4], [1, 3], [4, 3]], 4) 每隔两个数删除一个数有一个整型数组a[n]顺序存放0~n-1，要求每隔两个数删掉一个数，到末尾时循环至开头继续进行，求最后一个被删掉的数的原始下标位置。以8个数(n=8)为例:｛0，1，2，3，4，5，6，7｝，0-&gt;1-&gt;2(删除)-&gt;3-&gt;4-&gt;5(删除)-&gt;6-&gt;7-&gt;0(删除)，如此循环直到最后一个数被删除。 123"""生成一个循环链表，一个值记录当前索引，当索引能被2整除且不为0时，删除该值，知道链表只剩一个元素""" LeetCodeLeetCode[3]:Longest Substring Without Repeating CharactersGiven a string, find the length of the longest substring without repeating characters.Examples:Given “abcabcbb”, the answer is “abc”, which the length is 3.Given “bbbbb”, the answer is “b”, with the length of 1.Given “pwwkew”, the answer is “wke”, with the length of 3. Note that the answer must be a substring, “pwke” is a subsequence and not a substring. 12345678910111213def lengthOfLongestSubstring(s): start = maxLength = 0 usedChar = &#123;&#125; for i in range(len(s)): if s[i] in usedChar and start &lt;= usedChar[s[i]]: start = usedChar[s[i]] + 1 else: maxLength = max(maxLength, i - start + 1) usedChar[s[i]] = i return maxLength]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql实现小记]]></title>
    <url>%2F2017%2F06%2F24%2Fsql%E5%AE%9E%E7%8E%B0%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景项目及开源学习中经常有些sql需要写，其中一些sql逻辑比较简单，但是另外的一个还是要思考一段时间的，本文就是把这部分需要思考一段时间的sql记录下来，可以与大家分享的同时，也可以避免长期未使用而遗忘。 MySQL部分单表自关联分组排序累加1234567891011121314create table tmp1( os_type int comment '源系统', hours varchar(4) comment '时间-小时', count_num int comment '数量');insert into tmp1 values(1, '01', 600);insert into tmp1 values(1, '02', 500);insert into tmp1 values(1, '03', 400);insert into tmp1 values(1, '04', 300);insert into tmp1 values(1, '05', 200);insert into tmp1 values(1, '06', 100);insert into tmp1 values(2, '01', 200);insert into tmp1 values(2, '02', 100);insert into tmp1 values(3, '01', 100); 求表各个os_type(系统)按照hours(时间)排序，并且累加count_num的值。单表通过os_type自关联，用时间进行过滤，然后再分组。 123456789select t1.os_type , t1.hours , sum(t2.count_num)from tmp1 t1inner join tmp1 t2 on t1.os_type = t2.os_type-- 通过where排除不符合条件的内容where t1.hours &gt;= t2.hoursgroup by t1.os_type, t1.hours 使用临时变量部分很多从Oracle转来使用MySQL的同学都会抱怨，很多Oracle内置的函数Mysql都不支持，这部分主要讨论使用MySQL临时变量完成部分Oracle函数的功能。主要实现的函数有三种排序并标序号，row_number ()partition over, rank() partition over, dense_rank() partition over几个功能。 建表首先对将要测试的数据进行建表，运行下面建表及初始化数据的sql脚本 123456789101112131415161718192021222324--------------------------------------------------------- 创建测试表--------------------------------------------------------- drop table if exists employee;create table employee ( empid int comment '员工编号', deptid int comment '部门编号', salary int comment '工资');insert into employee values(8,50,6500);insert into employee values(14,50,6500);insert into employee values(12,20,6500);insert into employee values(1,10,5500);insert into employee values(7,40,44500);insert into employee values(3,20,1900);insert into employee values(5,40,6500);insert into employee values(4,20,4800);insert into employee values(13,20,4500);insert into employee values(2,10,4500);insert into employee values(10,30,4500);insert into employee values(11,20,4500);insert into employee values(9,50,7500);insert into employee values(9,50,4500);insert into employee values(6,40,14500); 查看表employee中的数据，select * from employee;可以看到如图： 三种排序标序号这部分主要针对MySQL中没有分组(partition)的排序，即对选择的所有记录进行排序，其中分为简单的标号排序，不跳过排序的并列值，跳过排序的并列值。sql实现难度也从简单到困难，下面对这三种标号的排序进行说明。 单纯标号排序单纯对工资排序，即使值重复序号也不一样，不会因为值重复就跳过重复的数量。 他的实现逻辑是编写一个按照要排序的字段(这里是salary字段)的升序后降序子查询t1，这样就能得到按照工资降序的临时子查询表t1。然后和临时变量@rank进行关联，每次在最外层查询select字段的时候，都将临时变量@rank的值加一。 1234567891011121314151617181920--------------------------------------------------------- 只按照工资从高到低排名-------------------------------------------------------select t1.empid , t1.deptid , t1.salary , @rank := @rank + 1 as rankfrom( select empid , deptid , salary from employee order by salary desc) t1inner join( select @rank := 0) t2; 运行上面的sql会看到下面的结果： 排序不跳过并列序号对工资进行排序，工资相同排序相同，有并列排名时不会跳过。在单纯标号排序的基础上比较每两个相邻的salary的值，当连续的salary相同时排名相同。 先按照要排序的字段(salary)进行升序或降序，生成一个临时表t1，然后和临时变量@rank(记录当前排名)，@pre_salary(记录上一条记录salary的值)进行关联。在最外层的查询中，表达式@pre_salary != (@pre_salary := salary)是判断当前的salary和@pre_salary是否相等，当两个值相等，是表达式为0，反之为1。所以外层查询选择的第四个字段相等于@rank = @rank + (0 or 1)，这样就能实现当salary相同时排名相同。 1234567891011121314151617181920212223--------------------------------------------------------- 对工资进行排序，工资相同排序相同，但不会跳过排名-------------------------------------------------------select t1.empid , t1.deptid , t1.salary -- @rank = @rank + (0 or 1) 如果表达式@pre_salary != (@pre_salary := salary)为True就是1 否则就是0 , @rank := @rank + (@pre_salary != (@pre_salary := salary)) as rankfrom( select empid , deptid , salary from employee order by salary desc) t1inner join( -- @rank记录当前的排名 @pre_salary记录salary的上一个值 select @rank := 0 , @pre_salary := -1) t2; 运行上面的sql会看到下面的结果： 排序跳过并列序号对工资进行排序，工资相同排序相同，有并列排名时会跳过。排序跳过并列序号是排序不跳过并列序号的升级版，其难点是要比较两个连续salary是否相等的同时，记录连续salary出现的次数@skip_num。当两个连续salary不相等的时候，@rank的值除了要加一外，还要加上@skip_num的值。 先生成按照salary降序的临时表t1，临时变量@rank用于计算当前排名，@skip_num用于记录连续相同salary的次数，@pre_salary_1和@pre_salary_2均用于判断当前记录salary和前一条记录的salary是否相等，前者为@rank服务，后者为@skip_num服务。 1234567891011121314151617181920212223242526272829303132333435363738--------------------------------------------------------- 对工资进行排序，工资相同排序相同，有并列排名时会跳过-------------------------------------------------------select t3.empid , t3.deptid , t3.salary , t3.rankfrom( select t1.empid , t1.deptid , t1.salary -- 当前值和之前的值一样，即@pre_salary_1 = (@pre_salary_1 := salary)时 取之前的排名@rank 不一样时取当前排名@rank+1+之前跳过的排名@skip_num , if ( @pre_salary_1 = (@pre_salary_1 := salary), @rank, @rank := @rank + 1 + @skip_num ) as rank -- @skip_num保存同一排名出现的次数 @pre_salary_2 = (@pre_salary_2 := salary)是就说明同一排名出现一次 , if ( @pre_salary_2 = (@pre_salary_2 := salary), @skip_num := @skip_num + 1, @skip_num := 0 ) as _skip_num from ( select empid , deptid , salary from employee order by salary desc ) t1 inner join ( -- @rank记录当前的排名 @pre_salary_1和@pre_salary_2记录salary的前一个值 @pre_salary_1用于计算排名是否增加 @pre_salary_2是中间变量确定排名跳过的数量，即@skip_num select @rank := 0 , @pre_salary_1 := -1 , @pre_salary_2 := -1 , @skip_num := 0 ) t2) t3; 运行上面的sql会看到下面的结果： 实现oracle相关分析函数功能这部分主要实现了Oracle相关的分析函数，包括row_number, rank, dense_rank三个分析函数。 partition row_number实现Oracle的row_number() partition over方法，要达到的目的是先对源数据按其中一个字段(deptid)进行分组，再在分组中按照另一个字段(salary)升序或降序排序，并进行标号。 其实现逻辑是，生成一张按照deptid和salary排序的临时表，然后和临时变量@pre_dept和@rank进行关联，其中@pre_dept表示前一条记录部门的值，@rank表示排名。主要的实现在外层查询的if (@pre_dept = (@pre_dept := deptid), @rank := @rank + 1, @rank := 1)中，如果当前deptid和前一条记录的deptid相同时，说明是在同一个deptid的分组中，排名@rank就增加1；如果当前的deptid和前一条记录的deptid不相同，说明deptid分组已经变化了，排名@rank要重新从1开始计算，所以@rank:=1。 1234567891011121314151617181920212223242526272829--------------------------------------------------------- Oracle row_number方法-- 按照部门分组 每组进行排序-- 排序逻辑 按工资高低排序 同样工资不同顺序-------------------------------------------------------select t1.empid , t1.deptid , t1.salary -- 如果当前deptid(即@pre_dept := deptid) != 之前的deptid(即@pre_dept) 就令当前排序初始化为1(即@rank := 1); -- 如果等于之前的deptid 就在原来的排序上加一(即@rank := @rank + 1) , if ( @pre_dept = (@pre_dept := deptid), @rank := @rank + 1, @rank := 1 ) as _row_numberfrom( -- 第一步 通过这里控制 以deptid分组 以salary降序 select empid , deptid , salary from employee order by deptid, salary desc) t1inner join( -- 此处要选一个异常值(不会在数据中出现的值) 尽量别用null 因为null有陷阱 select @pre_dept := -1 , @rank := -1) t2; 运行上面的sql会看到下面的结果： partition rank实现Oracle的rank() partition over方法，是row_number的升级版。要达到的目的是先对源数据按其中一个字段(deptid)进行分组，再在分组中按照另一个字段(salary)升序或降序排序，并进行排序，此时的排序允许有并列序号，但是不会跳过并列序号的次数。 最外层查询只是为了选择@pre_salary变量的值，主要逻辑集中在次外层查询。首先在employee表中按照部门和工资排序，得到子查询t1，再和@pre_dept(前一条记录的部门), @rank(排名), @pre_salary(前一条记录的工资)关联。在次外层查询中，满足当前部门(deptid)和前一条记录的部门(@pre_salary)相等，并且当前记录工资(salary)和前一条记录工资(@pre_salary)相等时，说明是同一个部门并且工资相同，所以排序(@rank)值不变；仅是部门相等但是工资不等，则说明同一部门但是工资不同，所以@rank + 1；回到sql次外层查询中最外层的if条件，当前部门和前一条记录的部门不相等时，则部门变了，排序要重新开始，所以@rank := 1。注意次外层查询中，单独@pre_salary := salary进行赋值，防止当外层if条件不满足时，在内层sql写@pre_salary = (@pre_salary := salary)不能更新@pre_salary的值。 123456789101112131415161718192021222324252627282930313233343536373839404142--------------------------------------------------------- 实现Oracle的rank() partition over方法-------------------------------------------------------select t3.empid , t3.deptid , t3.salary , t3._rank_overfrom( select t1.empid , t1.deptid , t1.salary -- 如果当前deptid(即@pre_dept := deptid) != 之前的deptid(即@pre_dept) 就令当前排序初始化为1(即@rank := 1); -- 如果等于之前的deptid 就在原来的排序上加一(即@rank := @rank + 1) , if ( @pre_dept = (@pre_dept := deptid) -- 如果部门相等 比较同部门的工资是否相等 , if ( @pre_salary = salary, @rank, @rank := @rank + 1 ) , @rank := 1 ) as _rank_over -- 单独保存本次salary的值 放在@pre_dept = (@pre_dept := deptid)里面会因为条件不一样导致结果不同 , @pre_salary := salary from ( -- 第一步 通过这里控制 以deptid分组 以salary降序 select empid , deptid , salary from employee order by deptid, salary desc ) t1 inner join ( -- 此处要选一个异常值(不会在数据中出现的值) 尽量别用null 因为null有陷阱 select @pre_dept := -1 , @rank := -1 , @pre_salary := -1 ) t2) t3; 运行上面的sql会看到下面的结果： partition dense_rank实现Oracle的dense_rank() partition over方法，是rank方法的升级版。在rank方法的基础上，如果有重复的序号，要跳过重复的次数。 先按照部门和工资排序，生成子查询t1。和临时变量@pre_dept(上一条记录部门的值)，@rank(当前数据排名)，@pre_salary(上一条记录工资的值)，@skip_num(该部门分组该排名需要跳过的值)进行关联。其中排名实现在rank方法上加上@skip_num值，所以剩下的问题是确定@skip_num的值。若当前工资的值和上一条记录中工资的值相等，@skip_num的值就加一，反之@skip_num均赋值为0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546--------------------------------------------------------- oracle dense_rank() over 方法-------------------------------------------------------select t3.empid , t3.deptid , t3.salary , t3._dense_rank_overfrom( select t1.empid , t1.deptid , t1.salary -- 如果当前deptid(即@pre_dept := deptid) != 之前的deptid(即@pre_dept) 就令当前排序初始化为1(即@rank := 1); -- 如果等于之前的deptid 就在原来的排序上加一(即@rank := @rank + 1) , if ( @pre_dept = (@pre_dept := deptid) -- 如果部门相等 比较同部门的工资是否相等 , if ( @pre_salary = salary, @rank, @rank := @rank + 1 + @skip_num ) , @rank := 1 ) as _dense_rank_over -- 单独保存本次salary的值 放在@pre_dept = (@pre_dept := deptid)里面会因为条件不一样导致结果不同 , if ( @pre_salary = salary, @skip_num := @skip_num + 1, @skip_num := 0 ) as _skip_num , @pre_salary := salary from ( -- 第一步 通过这里控制 以deptid分组 以salary降序 select empid , deptid , salary from employee order by deptid, salary desc ) t1 inner join ( -- 此处要选一个异常值(不会在数据中出现的值) 尽量别用null 因为null有陷阱 select @pre_dept := -1 , @rank := -1 , @pre_salary := -1 , @skip_num := 0 ) t2) t3; 运行上面的sql会看到下面的结果：]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql_schedule_backup定时备份mysql工具]]></title>
    <url>%2F2017%2F06%2F12%2Fmysql-schedule-backup%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BDmysql%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[背景mysql_schedule_backup是定时、全量备份指定mysql数据库的程序。可设置是否压缩备份文件、备份文件保留的天数。使用场景是简单的备份逻辑，通过schedule就能满足的备份逻辑，没有DAG流程。 Requirements schedule==0.4.2 How to run 先安装schedule 1pip install schedule 根据demo配置conf.py中相关的备份信息 修改mysql_schedule_backup.py文件中schedule.every().day.at(&quot;06:00&quot;).do(backup_mysql, bacup_conf)为你的业务时间 在mysql_schedule_backup目录下运行 1python mysql_schedule_backup.py 备份结果 备份是文件夹或者压缩文件，以备份时间为名称 文件夹（压缩文件）里面分为*.sql和*.json两类文件，其中conf.json为备份的配置文件，*.sql为备份的主文件，以datetime_database.sql方式命名 最终的备份样例 12345└─20170611 ├─20170611_sakila.sql ├─20170611_tmp.sql ├─20170611_world.sql └─conf.json Features 使用schedule支持定时备份 使用配置文件conf.py统一除了备份时间外的配置备份信息（schedule的定时计划比较复杂，没有放置到配置文件conf.py中，而是放在主程序mysql_schedule_backup.py） 支持多个数据库备份 支持压缩备份文件 支持设置备份文件保留时间 备份文件中包含mysql_schedule_backup运行的配置信息 项目构成1234└─mysql_schedule_backup ├─conf.py ├─mysql_schedule_backup.py └─requirements.txt requirements.txt:项目的requirements列表 mysql_schedule_backup.py:是备份运行的主程序，其中backup_mysql函数是与配置文件conf.py交互的主函数。如果定制配置文件的样式可以对backup_mysql函数进行相应的修改 12345678910if __name__ == "__main__": SLEEP_TIME = 30 # 测试定时运行程序 每分钟运行一次 # schedule.every().minutes.do(backup_mysql) # 每天固定时间运行程序 schedule.every().day.at("06:00").do(backup_mysql) while True: schedule.run_pending() time.sleep(SLEEP_TIME) conf.py:mysql_schedule_backup是定时的配置文件，可配置内容包括备份存放路径、备份保留时间、备份是否压缩、备份的数据库及表（支持多个库备份）。一般的备份文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243job = &#123; "setting": &#123; # 备份的根目录 "root": "F:/mysql_schedule_backup", # 备份保持的时间 "keep_day": 60, # 是否压缩备份 "is_zip": "true" &#125;, "db_connection": [ # 第一个要备份的库 &#123; "usr": "root", "pwd": "mysql", "host": "127.0.0.1", "port": "3306", "db_and_table": &#123; # "backup_database": ["backup_table_1", "backup_table_2", ...] "tmp": [ "tb1", "tb2" ], # "backup_database": ["*"] # *默认全库备份 "world": [ "*" ] &#125; &#125;, # 第二个要备份的库 &#123; "usr": "root", "pwd": "mysql", "host": "127.0.0.1", "port": "3306", "db_and_table": &#123; # "backup_database": ["backup_table_1", "backup_table_2", ...] "sakila": [ "*" ] &#125; &#125; ]&#125; 欢迎startmysql_schedule_backup地址，欢迎start或者fork]]></content>
      <categories>
        <category>mysql-backup</category>
      </categories>
      <tags>
        <tag>mysql-backup</tag>
        <tag>MySQL</tag>
        <tag>Python</tag>
        <tag>schedule</tag>
      </tags>
  </entry>
</search>
